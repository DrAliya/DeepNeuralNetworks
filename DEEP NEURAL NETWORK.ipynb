{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# 1. Packages\n",
    "#Let’s first import all the packages that you will need during this assignment.\n",
    "\n",
    "  #  numpy is the main package for scientific computing with Python.\n",
    "  #  matplotlib is a library to plot graphs in Python.\n",
    "   # dnn_utils provides some necessary functions for this notebook.\n",
    "  #  testCases provides some test cases to assess the correctness of your functions\n",
    "  #  np.random.seed(1) is used to keep all the random function calls consistent. It will help us grade your work. Please don’t change the seed.\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "#from testCases_v3 import *\n",
    "#from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0); # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the sigmoid function:\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "\n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = 1 / (1 + np.exp(-Z));\n",
    "    cache = Z;\n",
    "\n",
    "    return A, cache;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the sigmoid_backward function:\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "\n",
    "    Z = cache;\n",
    "\n",
    "    s = 1 / (1 + np.exp(-Z));\n",
    "    dZ = dA * s * (1 - s);\n",
    "\n",
    "    assert (dZ.shape == Z.shape);\n",
    "\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the relu function:\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "\n",
    "    A = np.maximum(0,Z);\n",
    "\n",
    "    assert(A.shape == Z.shape);\n",
    "\n",
    "    cache = Z; \n",
    "    return A, cache;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the relu_backward function：\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "\n",
    "    Z = cache;\n",
    "    dZ = np.array(dA, copy = True); # just converting dz to a correct object.\n",
    "\n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0;\n",
    "\n",
    "    assert (dZ.shape == Z.shape);\n",
    "\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.1 2-layer Neural Network\n",
    "# The model’s structure is: LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1);\n",
    "\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01;\n",
    "    b1 = np.zeros((n_h, 1));\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01;\n",
    "    b2 = np.zeros((n_y, 1));\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(W1.shape == (n_h, n_x));\n",
    "    assert(b1.shape == (n_h, 1));\n",
    "    assert(W2.shape == (n_y, n_h));\n",
    "    assert(b2.shape == (n_y, 1));\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2};\n",
    "\n",
    "    return parameters;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters = initialize_parameters(3,2,1);\n",
    "#print(\"W1 = \" + str(parameters[\"W1\"]));\n",
    "#print(\"b1 = \" + str(parameters[\"b1\"]));\n",
    "#print(\"W2 = \" + str(parameters[\"W2\"]));\n",
    "#print(\"b2 = \" + str(parameters[\"b2\"]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.2 L-layer Neural Network\n",
    "\n",
    "# GRADED FUNCTION: initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(3);\n",
    "    parameters = {};\n",
    "    L = len(layer_dims);     # number of layers in the network\n",
    "\n",
    "#    if L == 1:\n",
    "#parameters[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0])*0.01\n",
    "#parameters[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))\n",
    "#else\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01;\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1));\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]));\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1));\n",
    "\n",
    "\n",
    "    return parameters;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters = initialize_parameters_deep([5,4,3]);\n",
    "#print(\"W1 = \" + str(parameters[\"W1\"]));\n",
    "#print(\"b1 = \" + str(parameters[\"b1\"]));\n",
    "#print(\"W2 = \" + str(parameters[\"W2\"]));\n",
    "#print(\"b2 = \" + str(parameters[\"b2\"]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.1 Linear Forward\n",
    "# GRADED FUNCTION: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    Z = np.dot(W, A) + b;\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]));\n",
    "    cache = (A, W, b);\n",
    "\n",
    "    return Z, cache;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def linear_forward_test_case():\n",
    " #   np.random.seed(1);\n",
    "  #  A = np.random.randn(3,2);\n",
    "   # W = np.random.randn(1,3);\n",
    "    #b = np.random.randn(1,1);\n",
    "    #return A, W, b;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A, W, b = linear_forward_test_case();\n",
    "#Z, linear_cache = linear_forward(A, W, b);\n",
    "#print(\"Z = \" + str(Z));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.2 Linear-Activation Forward\n",
    "# GRADED FUNCTION: linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b); # Z, (W, A_prev, B)\n",
    "        A, activation_cache = sigmoid(Z); # A, (Z)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b); \n",
    "        A, activation_cache = relu(Z);\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]));\n",
    "    cache = (linear_cache, activation_cache); #, ((W, A_prev, B) ,(Z))\n",
    "\n",
    "    return A, cache;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def linear_activation_forward_test_case():\n",
    " #   np.random.seed(2)\n",
    "  #  A_prev = np.random.randn(3,2)\n",
    "   # W = np.random.randn(1,3)\n",
    "    #b = np.random.randn(1,1)\n",
    "    #return A_prev, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A_prev, W, b = linear_activation_forward_test_case();\n",
    "\n",
    "#A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\");\n",
    "#print(\"With sigmoid: A = \" + str(A));\n",
    "\n",
    "#A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\");\n",
    "#print(\"With ReLU: A = \" + str(A));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.3 L-Layer Model\n",
    "# GRADED FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "\n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "\n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "\n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A, linear_activation_cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], \"relu\");\n",
    "        caches.append(linear_activation_cache);\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    AL, linear_activation_cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], \"sigmoid\");\n",
    "    caches.append(linear_activation_cache);\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "  #  assert(AL.shape == (1,X.shape[1]));\n",
    "    \n",
    "    return AL, caches;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def L_model_forward_test_case():\n",
    " #   np.random.seed(1);\n",
    "  #  X = np.random.randn(4,2);\n",
    "   # W1 = np.random.randn(3,4);\n",
    "    #b1 = np.random.randn(3,1);\n",
    "    #W2 = np.random.randn(1,3);\n",
    "    #b2 = np.random.randn(1,1);\n",
    "    #parameters = {\"W1\": W1,\n",
    "     #             \"b1\": b1,\n",
    "      #            \"W2\": W2,\n",
    "       #           \"b2\": b2};\n",
    "\n",
    "    #return X, parameters;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, parameters = L_model_forward_test_case();\n",
    "#AL, caches = L_model_forward(X, parameters);\n",
    "#print(\"AL = \" + str(AL));\n",
    "#print(\"Length of caches list = \" + str(len(caches)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Cost function\n",
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "\n",
    "    m = Y.shape[1];\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    cost = -1 / m * (np.dot(Y, np.log(AL).T) + np.dot(1 - Y, np.log(1 - AL).T));\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "   # cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "   # assert(cost.shape == ())\n",
    "    \n",
    "    return cost;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_test_case(): \n",
    "    Y = np.asarray([[1, 1, 1]]);\n",
    "    aL = np.array([[.8,.9,0.4]]); \n",
    "    return Y, aL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = [[0.4149316]]\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case();\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.1 Linear backward\n",
    "# GRADED FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache;\n",
    "    m = A_prev.shape[1];\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    dW = 1 / m * np.dot(dZ, A_prev.T);\n",
    "    db = 1 / m * np.sum(dZ, axis = 1, keepdims = True);\n",
    "    dA_prev = np.dot(W.T, dZ);\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert (dA_prev.shape == A_prev.shape);\n",
    "    assert (dW.shape == W.shape);\n",
    "    assert (db.shape == b.shape);\n",
    "\n",
    "    return dA_prev, dW, db;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def linear_backward_test_case():\n",
    "  #  np.random.seed(1);\n",
    "   # dZ = np.random.randn(1,2);\n",
    "    #A = np.random.randn(3,2);\n",
    "    #W = np.random.randn(1,3);\n",
    "    #b = np.random.randn(1,1);\n",
    "    #linear_cache = (A, W, b);\n",
    "    #return dZ, linear_cache;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some test inputs\n",
    "#dZ, linear_cache = linear_backward_test_case();\n",
    "#dA_prev, dW, db = linear_backward(dZ, linear_cache);\n",
    "#print (\"dA_prev = \"+ str(dA_prev));\n",
    "#print (\"dW = \" + str(dW));\n",
    "#print (\"db = \" + str(db));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.2 Linear-Activation backward\n",
    "# GRADED FUNCTION: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = relu_backward(dA, activation_cache);\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache);\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = sigmoid_backward(dA, activation_cache);\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache);\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return dA_prev, dW, db;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def linear_activation_backward_test_case():\n",
    " #   np.random.seed(2);\n",
    "  #  dA = np.random.randn(1,2);\n",
    "   # A = np.random.randn(3,2);\n",
    "   # W = np.random.randn(1,3);\n",
    "   # b = np.random.randn(1,1);\n",
    "   # Z = np.random.randn(1,2);\n",
    "   # linear_cache = (A, W, b);\n",
    "  #  activation_cache = Z;\n",
    "  #  linear_activation_cache = (linear_cache, activation_cache);\n",
    "\n",
    "  #  return dA, linear_activation_cache;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AL, linear_activation_cache = linear_activation_backward_test_case();\n",
    "\n",
    "#dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"sigmoid\");\n",
    "#print (\"sigmoid:\");\n",
    "#print (\"dA_prev = \"+ str(dA_prev));\n",
    "#print (\"dW = \" + str(dW));\n",
    "#print (\"db = \" + str(db) + \"\\n\");\n",
    "\n",
    "#dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"relu\");\n",
    "##print (\"relu:\");\n",
    "#print (\"dA_prev = \"+ str(dA_prev));\n",
    "#print (\"dW = \" + str(dW));\n",
    "#print (\"db = \" + str(db));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.3 L-Model Backward\n",
    "# GRADED FUNCTION: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "\n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {};\n",
    "    L = len(caches); # the number of layers\n",
    "    m = AL.shape[1];\n",
    "    Y = Y.reshape(AL.shape); # after this line, Y is the same shape as AL\n",
    "\n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL));\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    dA_prev, dW, db = linear_activation_backward(dAL, caches[L - 1], \"sigmoid\");\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = dA_prev, dW, db;\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        dA = dA_prev;\n",
    "        dA_prev, dW, db = linear_activation_backward(dA, caches[l], \"relu\");\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev;\n",
    "        grads[\"dW\" + str(l + 1)] = dW;\n",
    "        grads[\"db\" + str(l + 1)] = db;\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def L_model_backward_test_case():\n",
    " #   \"\"\"\n",
    "  #  X = np.random.rand(3,2)\n",
    "   # Y = np.array([[1, 1]])\n",
    "    #parameters = {'W1': np.array([[ 1.78862847,  0.43650985,  0.09649747]]), 'b1': np.array([[ 0.]])}\n",
    "\n",
    "    #aL, caches = (np.array([[ 0.60298372,  0.87182628]]), [((np.array([[ 0.20445225,  0.87811744],\n",
    "     #      [ 0.02738759,  0.67046751],\n",
    "      #     [ 0.4173048 ,  0.55868983]]),\n",
    "    #np.array([[ 1.78862847,  0.43650985,  0.09649747]]),\n",
    "  #  np.array([[ 0.]])),\n",
    " #  np.array([[ 0.41791293,  1.91720367]]))])\n",
    "  # \"\"\"\n",
    " #   np.random.seed(3)\n",
    " #   AL = np.random.randn(1, 2)\n",
    " #   Y = np.array([[1, 0]])\n",
    "\n",
    " #   A1 = np.random.randn(4,2)\n",
    " #   W1 = np.random.randn(3,4)\n",
    " #   b1 = np.random.randn(3,1)\n",
    " #   Z1 = np.random.randn(3,2)\n",
    "  #  linear_cache_activation_1 = ((A1, W1, b1), Z1)\n",
    "\n",
    "  #  A2 = np.random.randn(3,2)\n",
    "  #  W2 = np.random.randn(1,3)\n",
    "  #  b2 = np.random.randn(1,1)\n",
    "  #  Z2 = np.random.randn(1,2)\n",
    "  #  linear_cache_activation_2 = ((A2, W2, b2), Z2)\n",
    "\n",
    "  #  caches = (linear_cache_activation_1, linear_cache_activation_2)\n",
    "\n",
    "  #  return AL, Y, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AL, Y_assess, caches = L_model_backward_test_case();\n",
    "#grads = L_model_backward(AL, Y_assess, caches);\n",
    "#print(grads);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.4 Update Parameters\n",
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] -= learning_rate * grads[\"dW\" + str(l + 1)];\n",
    "        parameters[\"b\" + str(l + 1)] -= learning_rate * grads[\"db\" + str(l + 1)];\n",
    "    ### END CODE HERE ###\n",
    "    return parameters;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def update_parameters_test_case():\n",
    " #   np.random.seed(2)\n",
    "  #  W1 = np.random.randn(3,4)\n",
    "   # b1 = np.random.randn(3,1)\n",
    " #   W2 = np.random.randn(1,3)\n",
    "  #  b2 = np.random.randn(1,1)\n",
    "  #  parameters = {\"W1\": W1,\n",
    "   #               \"b1\": b1,\n",
    "  #                \"W2\": W2,\n",
    "  #                \"b2\": b2}\n",
    " #   np.random.seed(3)\n",
    " #   dW1 = np.random.randn(3,4)\n",
    "  #  db1 = np.random.randn(3,1)\n",
    " #   dW2 = np.random.randn(1,3)\n",
    "  #  db2 = np.random.randn(1,1)\n",
    "  #  grads = {\"dW1\": dW1,\n",
    "  #           \"db1\": db1,\n",
    "    #         \"dW2\": dW2,\n",
    "    #         \"db2\": db2}\n",
    "\n",
    "   # return parameters, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters, grads = update_parameters_test_case();\n",
    "#parameters = update_parameters(parameters, grads, 0.1);\n",
    "\n",
    "#print (\"W1 = \"+ str(parameters[\"W1\"]));\n",
    "#print (\"b1 = \"+ str(parameters[\"b1\"]));\n",
    "#print (\"W2 = \"+ str(parameters[\"W2\"]));\n",
    "#print (\"b2 = \"+ str(parameters[\"b2\"]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "from sklearn import metrics\n",
    "p = pd.read_csv(r\"C:\\Users\\Admin\\Desktop\\PHY TESTER\\PYTHON\\dataset\\csv.csv\") # to read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>flag</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>count</th>\n",
       "      <th>srv_count</th>\n",
       "      <th>serror_rate</th>\n",
       "      <th>srv_serror_rate</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>ftp_data</td>\n",
       "      <td>SF</td>\n",
       "      <td>491</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>udp</td>\n",
       "      <td>other</td>\n",
       "      <td>SF</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>DOS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>232</td>\n",
       "      <td>8153</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>199</td>\n",
       "      <td>420</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  protocol_type   service flag  src_bytes  dst_bytes  logged_in  count  \\\n",
       "0           tcp  ftp_data   SF        491          0          0      2   \n",
       "1           udp     other   SF        146          0          0     13   \n",
       "2           tcp   private   S0          0          0          0    123   \n",
       "3           tcp      http   SF        232       8153          1      5   \n",
       "4           tcp      http   SF        199        420          1     30   \n",
       "\n",
       "   srv_count  serror_rate  srv_serror_rate  ...  dst_host_srv_count  \\\n",
       "0          2          0.0              0.0  ...                  25   \n",
       "1          1          0.0              0.0  ...                   1   \n",
       "2          6          1.0              1.0  ...                  26   \n",
       "3          5          0.2              0.2  ...                 255   \n",
       "4         32          0.0              0.0  ...                 255   \n",
       "\n",
       "   dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
       "0                    0.17                    0.03   \n",
       "1                    0.00                    0.60   \n",
       "2                    0.10                    0.05   \n",
       "3                    1.00                    0.00   \n",
       "4                    1.00                    0.00   \n",
       "\n",
       "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "0                         0.17                         0.00   \n",
       "1                         0.88                         0.00   \n",
       "2                         0.00                         0.00   \n",
       "3                         0.03                         0.04   \n",
       "4                         0.00                         0.00   \n",
       "\n",
       "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "0                  0.00                      0.00                  0.05   \n",
       "1                  0.00                      0.00                  0.00   \n",
       "2                  1.00                      1.00                  0.00   \n",
       "3                  0.03                      0.01                  0.00   \n",
       "4                  0.00                      0.00                  0.00   \n",
       "\n",
       "   dst_host_srv_rerror_rate   class  \n",
       "0                      0.00  normal  \n",
       "1                      0.00  normal  \n",
       "2                      0.00     DOS  \n",
       "3                      0.01  normal  \n",
       "4                      0.00  normal  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.head() # to see the first 5 lines of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 35 rows.\n"
     ]
    }
   ],
   "source": [
    "print(\"Read {} rows.\".format(len(p)))\n",
    "# df = df.sample(frac=0.1, replace=False) # Uncomment this line to sample only 10% of the dataset\n",
    "p.dropna(inplace=True,axis=1) # For now, just drop NA's (rows with missing values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   protocol_type     service flag  src_bytes  dst_bytes  logged_in  count  \\\n",
      "0            tcp    ftp_data   SF        491          0          0      2   \n",
      "1            udp       other   SF        146          0          0     13   \n",
      "2            tcp     private   S0          0          0          0    123   \n",
      "3            tcp        http   SF        232       8153          1      5   \n",
      "4            tcp        http   SF        199        420          1     30   \n",
      "5            tcp     private  REJ          0          0          0    121   \n",
      "6            tcp     private   S0          0          0          0    166   \n",
      "7            tcp     private   S0          0          0          0    117   \n",
      "8            tcp  remote_job   S0          0          0          0    270   \n",
      "9            tcp     private   S0          0          0          0    133   \n",
      "10           tcp     private  REJ          0          0          0    205   \n",
      "11           tcp     private   S0          0          0          0    199   \n",
      "12           tcp        http   SF        287       2251          1      3   \n",
      "13           tcp        name   S0          0          0          0    233   \n",
      "14           tcp  netbios_ns   S0          0          0          0     96   \n",
      "15           tcp        http   SF        300      13788          1      8   \n",
      "16           tcp        http   SF        233        616          1      3   \n",
      "17           tcp        http   SF        343       1178          1      9   \n",
      "18           tcp         mtp   S0          0          0          0    223   \n",
      "19           tcp     private   S0          0          0          0    280   \n",
      "20           tcp        http   SF        253      11905          1      8   \n",
      "21           udp       other   SF        147        105          0      1   \n",
      "22           tcp         mtp   S0          0          0          0    248   \n",
      "23           tcp      telnet   SF        437      14421          1      1   \n",
      "24           tcp     private   S0          0          0          0    279   \n",
      "25           tcp        http   SF        227       6588          1      5   \n",
      "26           tcp        http   SF        215      10499          1     14   \n",
      "27           tcp        http   SF        241       1400          1     33   \n",
      "28           tcp      finger   S0          0          0          0     57   \n",
      "29           tcp        http   SF        303        555          1      9   \n",
      "30           udp    domain_u   SF         45         45          0    181   \n",
      "31           udp     private   SF        105        147          0      2   \n",
      "32           udp    domain_u   SF         43         43          0    122   \n",
      "33           tcp      supdup   S0          0          0          0    205   \n",
      "34           tcp        http   SF        324       2302          1     22   \n",
      "\n",
      "    srv_count  serror_rate  srv_serror_rate  ...  dst_host_srv_count  \\\n",
      "0           2          0.0             0.00  ...                  25   \n",
      "1           1          0.0             0.00  ...                   1   \n",
      "2           6          1.0             1.00  ...                  26   \n",
      "3           5          0.2             0.20  ...                 255   \n",
      "4          32          0.0             0.00  ...                 255   \n",
      "5          19          0.0             0.00  ...                  19   \n",
      "6           9          1.0             1.00  ...                   9   \n",
      "7          16          1.0             1.00  ...                  15   \n",
      "8          23          1.0             1.00  ...                  23   \n",
      "9           8          1.0             1.00  ...                  13   \n",
      "10         12          0.0             0.00  ...                  12   \n",
      "11          3          1.0             1.00  ...                  13   \n",
      "12          7          0.0             0.00  ...                 219   \n",
      "13          1          1.0             1.00  ...                   1   \n",
      "14         16          1.0             1.00  ...                   2   \n",
      "15          9          0.0             0.11  ...                 255   \n",
      "16          3          0.0             0.00  ...                 255   \n",
      "17         10          0.0             0.00  ...                 255   \n",
      "18         23          1.0             1.00  ...                  23   \n",
      "19         17          1.0             1.00  ...                  17   \n",
      "20         10          0.0             0.00  ...                 255   \n",
      "21          1          0.0             0.00  ...                   1   \n",
      "22          2          1.0             1.00  ...                   2   \n",
      "23          1          0.0             0.00  ...                  25   \n",
      "24          7          1.0             1.00  ...                  13   \n",
      "25         22          0.0             0.00  ...                 255   \n",
      "26         14          0.0             0.00  ...                 255   \n",
      "27         33          0.0             0.00  ...                 255   \n",
      "28         16          1.0             1.00  ...                  59   \n",
      "29          9          0.0             0.00  ...                 255   \n",
      "30        181          0.0             0.00  ...                 250   \n",
      "31          2          0.0             0.00  ...                   5   \n",
      "32        202          0.0             0.00  ...                 255   \n",
      "33         17          1.0             1.00  ...                  17   \n",
      "34         28          0.0             0.00  ...                 255   \n",
      "\n",
      "    dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
      "0                     0.17                    0.03   \n",
      "1                     0.00                    0.60   \n",
      "2                     0.10                    0.05   \n",
      "3                     1.00                    0.00   \n",
      "4                     1.00                    0.00   \n",
      "5                     0.07                    0.07   \n",
      "6                     0.04                    0.05   \n",
      "7                     0.06                    0.07   \n",
      "8                     0.09                    0.05   \n",
      "9                     0.05                    0.06   \n",
      "10                    0.05                    0.07   \n",
      "11                    0.05                    0.07   \n",
      "12                    1.00                    0.00   \n",
      "13                    0.00                    0.07   \n",
      "14                    0.01                    0.06   \n",
      "15                    1.00                    0.00   \n",
      "16                    1.00                    0.00   \n",
      "17                    1.00                    0.00   \n",
      "18                    0.09                    0.05   \n",
      "19                    0.07                    0.06   \n",
      "20                    1.00                    0.00   \n",
      "21                    0.00                    0.85   \n",
      "22                    0.01                    0.06   \n",
      "23                    0.10                    0.05   \n",
      "24                    0.05                    0.07   \n",
      "25                    1.00                    0.00   \n",
      "26                    1.00                    0.00   \n",
      "27                    1.00                    0.00   \n",
      "28                    0.23                    0.04   \n",
      "29                    1.00                    0.00   \n",
      "30                    0.98                    0.01   \n",
      "31                    0.12                    0.05   \n",
      "32                    1.00                    0.00   \n",
      "33                    0.07                    0.07   \n",
      "34                    1.00                    0.00   \n",
      "\n",
      "    dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
      "0                          0.17                         0.00   \n",
      "1                          0.88                         0.00   \n",
      "2                          0.00                         0.00   \n",
      "3                          0.03                         0.04   \n",
      "4                          0.00                         0.00   \n",
      "5                          0.00                         0.00   \n",
      "6                          0.00                         0.00   \n",
      "7                          0.00                         0.00   \n",
      "8                          0.00                         0.00   \n",
      "9                          0.00                         0.00   \n",
      "10                         0.00                         0.00   \n",
      "11                         0.00                         0.00   \n",
      "12                         0.12                         0.03   \n",
      "13                         0.00                         0.00   \n",
      "14                         0.00                         0.00   \n",
      "15                         0.01                         0.02   \n",
      "16                         0.02                         0.03   \n",
      "17                         0.01                         0.04   \n",
      "18                         0.00                         0.00   \n",
      "19                         0.00                         0.00   \n",
      "20                         0.01                         0.02   \n",
      "21                         1.00                         0.00   \n",
      "22                         0.00                         0.00   \n",
      "23                         0.00                         0.00   \n",
      "24                         0.00                         0.00   \n",
      "25                         0.02                         0.14   \n",
      "26                         0.00                         0.00   \n",
      "27                         0.00                         0.00   \n",
      "28                         0.01                         0.00   \n",
      "29                         0.11                         0.01   \n",
      "30                         0.00                         0.00   \n",
      "31                         0.05                         0.00   \n",
      "32                         0.01                         0.00   \n",
      "33                         0.00                         0.00   \n",
      "34                         0.00                         0.00   \n",
      "\n",
      "    dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
      "0                   0.00                      0.00                  0.05   \n",
      "1                   0.00                      0.00                  0.00   \n",
      "2                   1.00                      1.00                  0.00   \n",
      "3                   0.03                      0.01                  0.00   \n",
      "4                   0.00                      0.00                  0.00   \n",
      "5                   0.00                      0.00                  1.00   \n",
      "6                   1.00                      1.00                  0.00   \n",
      "7                   1.00                      1.00                  0.00   \n",
      "8                   1.00                      1.00                  0.00   \n",
      "9                   1.00                      1.00                  0.00   \n",
      "10                  0.00                      0.00                  1.00   \n",
      "11                  1.00                      1.00                  0.00   \n",
      "12                  0.00                      0.00                  0.00   \n",
      "13                  1.00                      1.00                  0.00   \n",
      "14                  1.00                      1.00                  0.00   \n",
      "15                  0.00                      0.00                  0.00   \n",
      "16                  0.00                      0.00                  0.02   \n",
      "17                  0.00                      0.00                  0.00   \n",
      "18                  1.00                      1.00                  0.00   \n",
      "19                  0.99                      1.00                  0.00   \n",
      "20                  0.00                      0.00                  0.00   \n",
      "21                  0.00                      0.00                  0.00   \n",
      "22                  1.00                      1.00                  0.00   \n",
      "23                  0.53                      0.00                  0.02   \n",
      "24                  1.00                      1.00                  0.00   \n",
      "25                  0.00                      0.00                  0.56   \n",
      "26                  0.00                      0.00                  0.00   \n",
      "27                  0.00                      0.00                  0.00   \n",
      "28                  1.00                      1.00                  0.00   \n",
      "29                  0.00                      0.00                  0.00   \n",
      "30                  0.00                      0.00                  0.00   \n",
      "31                  0.00                      0.00                  0.00   \n",
      "32                  0.00                      0.00                  0.00   \n",
      "33                  1.00                      1.00                  0.00   \n",
      "34                  0.00                      0.00                  0.00   \n",
      "\n",
      "    dst_host_srv_rerror_rate   class  \n",
      "0                       0.00  normal  \n",
      "1                       0.00  normal  \n",
      "2                       0.00     DOS  \n",
      "3                       0.01  normal  \n",
      "4                       0.00  normal  \n",
      "5                       1.00     DOS  \n",
      "6                       0.00     DOS  \n",
      "7                       0.00     DOS  \n",
      "8                       0.00     DOS  \n",
      "9                       0.00     DOS  \n",
      "10                      1.00     DOS  \n",
      "11                      0.00     DOS  \n",
      "12                      0.00  normal  \n",
      "13                      0.00     DOS  \n",
      "14                      0.00     DOS  \n",
      "15                      0.00  normal  \n",
      "16                      0.00  normal  \n",
      "17                      0.00  normal  \n",
      "18                      0.00     DOS  \n",
      "19                      0.00     DOS  \n",
      "20                      0.00  normal  \n",
      "21                      0.00  normal  \n",
      "22                      0.00     DOS  \n",
      "23                      0.16  normal  \n",
      "24                      0.00     DOS  \n",
      "25                      0.57  normal  \n",
      "26                      0.00  normal  \n",
      "27                      0.00  normal  \n",
      "28                      0.00     DOS  \n",
      "29                      0.00  normal  \n",
      "30                      0.00  normal  \n",
      "31                      0.00  normal  \n",
      "32                      0.00  normal  \n",
      "33                      0.00     DOS  \n",
      "34                      0.00  normal  \n",
      "\n",
      "[35 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODING = 'utf-8'\n",
    "\n",
    "def expand_categories(values):\n",
    "    result = []\n",
    "    s = values.value_counts()\n",
    "    t = float(len(values))\n",
    "    for v in s.index:\n",
    "        result.append(\"{}:{}%\".format(v,round(100*(s[v]/t),2)))\n",
    "    return \"[{}]\".format(\",\".join(result))\n",
    "        \n",
    "def analyze(p):\n",
    "    print()\n",
    "    cols = p.columns.values\n",
    "    total = float(len(p))\n",
    "\n",
    "    print(\"{} rows\".format(int(total)))\n",
    "    for col in cols:\n",
    "        uniques = p[col].unique()\n",
    "        unique_count = len(uniques)\n",
    "        if unique_count>100:\n",
    "            print(\"** {}:{} ({}%)\".format(col,unique_count,int(((unique_count)/total)*100)))\n",
    "        else:\n",
    "            print(\"** {}:{}\".format(col,expand_categories(p[col])))\n",
    "            expand_categories(p[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(p, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = p[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = p[name].std()\n",
    "\n",
    "    p[name] = (p[name] - mean) / sd\n",
    "    \n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(p, name):\n",
    "    dummies = pd.get_dummies(p[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = f\"{name}-{x}\"\n",
    "        p[dummy_name] = dummies[x]\n",
    "    p.drop(name, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now encode the feature vector\n",
    "\n",
    "encode_text_dummy(p, 'protocol_type')\n",
    "encode_text_dummy(p, 'service')\n",
    "encode_text_dummy(p, 'flag')\n",
    "encode_numeric_zscore(p, 'src_bytes')\n",
    "encode_numeric_zscore(p, 'dst_bytes')\n",
    "encode_numeric_zscore(p, 'logged_in')\n",
    "encode_numeric_zscore(p, 'count')\n",
    "encode_numeric_zscore(p, 'srv_count')\n",
    "encode_numeric_zscore(p, 'serror_rate')\n",
    "encode_numeric_zscore(p, 'srv_serror_rate')\n",
    "encode_numeric_zscore(p, 'rerror_rate')\n",
    "encode_numeric_zscore(p, 'srv_rerror_rate')\n",
    "encode_numeric_zscore(p, 'same_srv_rate')\n",
    "encode_numeric_zscore(p, 'diff_srv_rate')\n",
    "encode_numeric_zscore(p, 'srv_diff_host_rate')\n",
    "encode_numeric_zscore(p, 'dst_host_count')\n",
    "encode_numeric_zscore(p, 'dst_host_srv_count')\n",
    "encode_numeric_zscore(p, 'dst_host_same_srv_rate')\n",
    "encode_numeric_zscore(p, 'dst_host_diff_srv_rate')\n",
    "encode_numeric_zscore(p, 'dst_host_same_src_port_rate')\n",
    "encode_numeric_zscore(p, 'dst_host_srv_diff_host_rate')\n",
    "encode_numeric_zscore(p, 'dst_host_serror_rate')\n",
    "encode_numeric_zscore(p, 'dst_host_srv_serror_rate')\n",
    "encode_numeric_zscore(p, 'dst_host_rerror_rate')\n",
    "encode_numeric_zscore(p, 'dst_host_srv_rerror_rate')\n",
    "\n",
    "# display 5 rows\n",
    "\n",
    "p.dropna(inplace=True,axis=1)\n",
    "p[0:5]\n",
    "# This is the numeric feature vector, as it goes to the neural net\n",
    "\n",
    "\n",
    "# Convert to numpy - Classification\n",
    "x_columns = p.columns.drop('class')\n",
    "x = p[x_columns].values\n",
    "dummies = pd.get_dummies(p['class']) # Classification\n",
    "outcomes = dummies.columns\n",
    "num_classes = len(outcomes)\n",
    "y = dummies.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "DOS       16\n",
       "normal    19\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.groupby('class')['class'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26 samples, validate on 9 samples\n",
      "Epoch 1/1000\n",
      "26/26 - 0s - loss: 0.6933 - val_loss: 0.6927\n",
      "Epoch 2/1000\n",
      "26/26 - 0s - loss: 0.6929 - val_loss: 0.6922\n",
      "Epoch 3/1000\n",
      "26/26 - 0s - loss: 0.6924 - val_loss: 0.6917\n",
      "Epoch 4/1000\n",
      "26/26 - 0s - loss: 0.6920 - val_loss: 0.6912\n",
      "Epoch 5/1000\n",
      "26/26 - 0s - loss: 0.6916 - val_loss: 0.6907\n",
      "Epoch 6/1000\n",
      "26/26 - 0s - loss: 0.6912 - val_loss: 0.6902\n",
      "Epoch 7/1000\n",
      "26/26 - 0s - loss: 0.6908 - val_loss: 0.6897\n",
      "Epoch 8/1000\n",
      "26/26 - 0s - loss: 0.6904 - val_loss: 0.6891\n",
      "Epoch 9/1000\n",
      "26/26 - 0s - loss: 0.6899 - val_loss: 0.6885\n",
      "Epoch 10/1000\n",
      "26/26 - 0s - loss: 0.6894 - val_loss: 0.6878\n",
      "Epoch 11/1000\n",
      "26/26 - 0s - loss: 0.6888 - val_loss: 0.6871\n",
      "Epoch 12/1000\n",
      "26/26 - 0s - loss: 0.6882 - val_loss: 0.6862\n",
      "Epoch 13/1000\n",
      "26/26 - 0s - loss: 0.6875 - val_loss: 0.6853\n",
      "Epoch 14/1000\n",
      "26/26 - 0s - loss: 0.6867 - val_loss: 0.6843\n",
      "Epoch 15/1000\n",
      "26/26 - 0s - loss: 0.6858 - val_loss: 0.6832\n",
      "Epoch 16/1000\n",
      "26/26 - 0s - loss: 0.6849 - val_loss: 0.6820\n",
      "Epoch 17/1000\n",
      "26/26 - 0s - loss: 0.6839 - val_loss: 0.6807\n",
      "Epoch 18/1000\n",
      "26/26 - 0s - loss: 0.6827 - val_loss: 0.6792\n",
      "Epoch 19/1000\n",
      "26/26 - 0s - loss: 0.6815 - val_loss: 0.6775\n",
      "Epoch 20/1000\n",
      "26/26 - 0s - loss: 0.6800 - val_loss: 0.6756\n",
      "Epoch 21/1000\n",
      "26/26 - 0s - loss: 0.6785 - val_loss: 0.6736\n",
      "Epoch 22/1000\n",
      "26/26 - 0s - loss: 0.6767 - val_loss: 0.6715\n",
      "Epoch 23/1000\n",
      "26/26 - 0s - loss: 0.6748 - val_loss: 0.6691\n",
      "Epoch 24/1000\n",
      "26/26 - 0s - loss: 0.6728 - val_loss: 0.6665\n",
      "Epoch 25/1000\n",
      "26/26 - 0s - loss: 0.6705 - val_loss: 0.6637\n",
      "Epoch 26/1000\n",
      "26/26 - 0s - loss: 0.6681 - val_loss: 0.6606\n",
      "Epoch 27/1000\n",
      "26/26 - 0s - loss: 0.6654 - val_loss: 0.6573\n",
      "Epoch 28/1000\n",
      "26/26 - 0s - loss: 0.6624 - val_loss: 0.6535\n",
      "Epoch 29/1000\n",
      "26/26 - 0s - loss: 0.6592 - val_loss: 0.6495\n",
      "Epoch 30/1000\n",
      "26/26 - 0s - loss: 0.6556 - val_loss: 0.6451\n",
      "Epoch 31/1000\n",
      "26/26 - 0s - loss: 0.6518 - val_loss: 0.6403\n",
      "Epoch 32/1000\n",
      "26/26 - 0s - loss: 0.6475 - val_loss: 0.6350\n",
      "Epoch 33/1000\n",
      "26/26 - 0s - loss: 0.6429 - val_loss: 0.6293\n",
      "Epoch 34/1000\n",
      "26/26 - 0s - loss: 0.6379 - val_loss: 0.6231\n",
      "Epoch 35/1000\n",
      "26/26 - 0s - loss: 0.6325 - val_loss: 0.6164\n",
      "Epoch 36/1000\n",
      "26/26 - 0s - loss: 0.6267 - val_loss: 0.6091\n",
      "Epoch 37/1000\n",
      "26/26 - 0s - loss: 0.6203 - val_loss: 0.6013\n",
      "Epoch 38/1000\n",
      "26/26 - 0s - loss: 0.6134 - val_loss: 0.5928\n",
      "Epoch 39/1000\n",
      "26/26 - 0s - loss: 0.6060 - val_loss: 0.5837\n",
      "Epoch 40/1000\n",
      "26/26 - 0s - loss: 0.5980 - val_loss: 0.5739\n",
      "Epoch 41/1000\n",
      "26/26 - 0s - loss: 0.5894 - val_loss: 0.5635\n",
      "Epoch 42/1000\n",
      "26/26 - 0s - loss: 0.5802 - val_loss: 0.5523\n",
      "Epoch 43/1000\n",
      "26/26 - 0s - loss: 0.5703 - val_loss: 0.5404\n",
      "Epoch 44/1000\n",
      "26/26 - 0s - loss: 0.5597 - val_loss: 0.5276\n",
      "Epoch 45/1000\n",
      "26/26 - 0s - loss: 0.5485 - val_loss: 0.5142\n",
      "Epoch 46/1000\n",
      "26/26 - 0s - loss: 0.5365 - val_loss: 0.4999\n",
      "Epoch 47/1000\n",
      "26/26 - 0s - loss: 0.5238 - val_loss: 0.4850\n",
      "Epoch 48/1000\n",
      "26/26 - 0s - loss: 0.5104 - val_loss: 0.4692\n",
      "Epoch 49/1000\n",
      "26/26 - 0s - loss: 0.4962 - val_loss: 0.4527\n",
      "Epoch 50/1000\n",
      "26/26 - 0s - loss: 0.4814 - val_loss: 0.4356\n",
      "Epoch 51/1000\n",
      "26/26 - 0s - loss: 0.4658 - val_loss: 0.4178\n",
      "Epoch 52/1000\n",
      "26/26 - 0s - loss: 0.4495 - val_loss: 0.3994\n",
      "Epoch 53/1000\n",
      "26/26 - 0s - loss: 0.4327 - val_loss: 0.3805\n",
      "Epoch 54/1000\n",
      "26/26 - 0s - loss: 0.4153 - val_loss: 0.3612\n",
      "Epoch 55/1000\n",
      "26/26 - 0s - loss: 0.3973 - val_loss: 0.3416\n",
      "Epoch 56/1000\n",
      "26/26 - 0s - loss: 0.3790 - val_loss: 0.3218\n",
      "Epoch 57/1000\n",
      "26/26 - 0s - loss: 0.3603 - val_loss: 0.3019\n",
      "Epoch 58/1000\n",
      "26/26 - 0s - loss: 0.3413 - val_loss: 0.2821\n",
      "Epoch 59/1000\n",
      "26/26 - 0s - loss: 0.3222 - val_loss: 0.2624\n",
      "Epoch 60/1000\n",
      "26/26 - 0s - loss: 0.3031 - val_loss: 0.2430\n",
      "Epoch 61/1000\n",
      "26/26 - 0s - loss: 0.2841 - val_loss: 0.2241\n",
      "Epoch 62/1000\n",
      "26/26 - 0s - loss: 0.2652 - val_loss: 0.2057\n",
      "Epoch 63/1000\n",
      "26/26 - 0s - loss: 0.2467 - val_loss: 0.1880\n",
      "Epoch 64/1000\n",
      "26/26 - 0s - loss: 0.2286 - val_loss: 0.1711\n",
      "Epoch 65/1000\n",
      "26/26 - 0s - loss: 0.2111 - val_loss: 0.1550\n",
      "Epoch 66/1000\n",
      "26/26 - 0s - loss: 0.1942 - val_loss: 0.1399\n",
      "Epoch 67/1000\n",
      "26/26 - 0s - loss: 0.1780 - val_loss: 0.1257\n",
      "Epoch 68/1000\n",
      "26/26 - 0s - loss: 0.1626 - val_loss: 0.1126\n",
      "Epoch 69/1000\n",
      "26/26 - 0s - loss: 0.1481 - val_loss: 0.1004\n",
      "Epoch 70/1000\n",
      "26/26 - 0s - loss: 0.1345 - val_loss: 0.0893\n",
      "Epoch 71/1000\n",
      "26/26 - 0s - loss: 0.1218 - val_loss: 0.0792\n",
      "Epoch 72/1000\n",
      "26/26 - 0s - loss: 0.1101 - val_loss: 0.0701\n",
      "Epoch 73/1000\n",
      "26/26 - 0s - loss: 0.0992 - val_loss: 0.0618\n",
      "Epoch 74/1000\n",
      "26/26 - 0s - loss: 0.0893 - val_loss: 0.0545\n",
      "Epoch 75/1000\n",
      "26/26 - 0s - loss: 0.0802 - val_loss: 0.0479\n",
      "Epoch 76/1000\n",
      "26/26 - 0s - loss: 0.0720 - val_loss: 0.0421\n",
      "Epoch 77/1000\n",
      "26/26 - 0s - loss: 0.0646 - val_loss: 0.0370\n",
      "Epoch 78/1000\n",
      "26/26 - 0s - loss: 0.0580 - val_loss: 0.0325\n",
      "Epoch 79/1000\n",
      "26/26 - 0s - loss: 0.0520 - val_loss: 0.0286\n",
      "Epoch 80/1000\n",
      "26/26 - 0s - loss: 0.0467 - val_loss: 0.0252\n",
      "Epoch 81/1000\n",
      "26/26 - 0s - loss: 0.0419 - val_loss: 0.0222\n",
      "Epoch 82/1000\n",
      "26/26 - 0s - loss: 0.0377 - val_loss: 0.0196\n",
      "Epoch 83/1000\n",
      "26/26 - 0s - loss: 0.0340 - val_loss: 0.0174\n",
      "Epoch 84/1000\n",
      "26/26 - 0s - loss: 0.0306 - val_loss: 0.0154\n",
      "Epoch 85/1000\n",
      "26/26 - 0s - loss: 0.0277 - val_loss: 0.0137\n",
      "Epoch 86/1000\n",
      "26/26 - 0s - loss: 0.0250 - val_loss: 0.0122\n",
      "Epoch 87/1000\n",
      "26/26 - 0s - loss: 0.0227 - val_loss: 0.0109\n",
      "Epoch 88/1000\n",
      "26/26 - 0s - loss: 0.0207 - val_loss: 0.0098\n",
      "Epoch 89/1000\n",
      "26/26 - 0s - loss: 0.0188 - val_loss: 0.0088\n",
      "Epoch 90/1000\n",
      "26/26 - 0s - loss: 0.0172 - val_loss: 0.0079\n",
      "Epoch 91/1000\n",
      "26/26 - 0s - loss: 0.0158 - val_loss: 0.0072\n",
      "Epoch 92/1000\n",
      "26/26 - 0s - loss: 0.0145 - val_loss: 0.0065\n",
      "Epoch 93/1000\n",
      "26/26 - 0s - loss: 0.0133 - val_loss: 0.0059\n",
      "Epoch 94/1000\n",
      "26/26 - 0s - loss: 0.0123 - val_loss: 0.0054\n",
      "Epoch 95/1000\n",
      "26/26 - 0s - loss: 0.0114 - val_loss: 0.0050\n",
      "Epoch 96/1000\n",
      "26/26 - 0s - loss: 0.0106 - val_loss: 0.0046\n",
      "Epoch 97/1000\n",
      "26/26 - 0s - loss: 0.0098 - val_loss: 0.0042\n",
      "Epoch 98/1000\n",
      "26/26 - 0s - loss: 0.0092 - val_loss: 0.0039\n",
      "Epoch 99/1000\n",
      "26/26 - 0s - loss: 0.0086 - val_loss: 0.0036\n",
      "Epoch 100/1000\n",
      "26/26 - 0s - loss: 0.0081 - val_loss: 0.0034\n",
      "Epoch 101/1000\n",
      "26/26 - 0s - loss: 0.0076 - val_loss: 0.0032\n",
      "Epoch 102/1000\n",
      "26/26 - 0s - loss: 0.0071 - val_loss: 0.0030\n",
      "Epoch 103/1000\n",
      "26/26 - 0s - loss: 0.0068 - val_loss: 0.0028\n",
      "Epoch 104/1000\n",
      "26/26 - 0s - loss: 0.0064 - val_loss: 0.0026\n",
      "Epoch 105/1000\n",
      "26/26 - 0s - loss: 0.0061 - val_loss: 0.0025\n",
      "Epoch 106/1000\n",
      "26/26 - 0s - loss: 0.0058 - val_loss: 0.0023\n",
      "Epoch 00106: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22032835390>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Create a test/train split.  25% test\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create neural net\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(50, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Measure accuracy\n",
    "pred = model.predict(x_test)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_eval = np.argmax(y_test,axis=1)\n",
    "score = metrics.accuracy_score(y_eval, pred)\n",
    "print(\"Validation score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18 samples, validate on 6 samples\n",
      "Epoch 1/1000\n",
      "18/18 - 0s - loss: 0.6931 - val_loss: 0.6933\n",
      "Epoch 2/1000\n",
      "18/18 - 0s - loss: 0.6931 - val_loss: 0.6934\n",
      "Epoch 3/1000\n",
      "18/18 - 0s - loss: 0.6931 - val_loss: 0.6933\n",
      "Epoch 4/1000\n",
      "18/18 - 0s - loss: 0.6931 - val_loss: 0.6932\n",
      "Epoch 5/1000\n",
      "18/18 - 0s - loss: 0.6931 - val_loss: 0.6930\n",
      "Epoch 6/1000\n",
      "18/18 - 0s - loss: 0.6931 - val_loss: 0.6930\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x220342e7cc0>"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Create a test/train split.  25% test\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create neural net\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='sigmoid'))\n",
    "model.add(Dense(50, input_dim=x.shape[1], kernel_initializer='normal', activation='sigmoid'))\n",
    "model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score: 0.5555555555555556\n"
     ]
    }
   ],
   "source": [
    "# Measure accuracy\n",
    "pred = model.predict(x_test)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_eval = np.argmax(y_test,axis=1)\n",
    "score = metrics.accuracy_score(y_eval, pred)\n",
    "print(\"Validation score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18 samples, validate on 6 samples\n",
      "Epoch 1/1000\n",
      "18/18 - 0s - loss: 0.6938 - val_loss: 0.6936\n",
      "Epoch 2/1000\n",
      "18/18 - 0s - loss: 0.6928 - val_loss: 0.6920\n",
      "Epoch 3/1000\n",
      "18/18 - 0s - loss: 0.6917 - val_loss: 0.6905\n",
      "Epoch 4/1000\n",
      "18/18 - 0s - loss: 0.6907 - val_loss: 0.6893\n",
      "Epoch 5/1000\n",
      "18/18 - 0s - loss: 0.6897 - val_loss: 0.6884\n",
      "Epoch 6/1000\n",
      "18/18 - 0s - loss: 0.6885 - val_loss: 0.6875\n",
      "Epoch 7/1000\n",
      "18/18 - 0s - loss: 0.6872 - val_loss: 0.6865\n",
      "Epoch 8/1000\n",
      "18/18 - 0s - loss: 0.6858 - val_loss: 0.6851\n",
      "Epoch 9/1000\n",
      "18/18 - 0s - loss: 0.6842 - val_loss: 0.6834\n",
      "Epoch 10/1000\n",
      "18/18 - 0s - loss: 0.6823 - val_loss: 0.6813\n",
      "Epoch 11/1000\n",
      "18/18 - 0s - loss: 0.6802 - val_loss: 0.6788\n",
      "Epoch 12/1000\n",
      "18/18 - 0s - loss: 0.6777 - val_loss: 0.6761\n",
      "Epoch 13/1000\n",
      "18/18 - 0s - loss: 0.6749 - val_loss: 0.6731\n",
      "Epoch 14/1000\n",
      "18/18 - 0s - loss: 0.6717 - val_loss: 0.6698\n",
      "Epoch 15/1000\n",
      "18/18 - 0s - loss: 0.6681 - val_loss: 0.6661\n",
      "Epoch 16/1000\n",
      "18/18 - 0s - loss: 0.6639 - val_loss: 0.6620\n",
      "Epoch 17/1000\n",
      "18/18 - 0s - loss: 0.6592 - val_loss: 0.6572\n",
      "Epoch 18/1000\n",
      "18/18 - 0s - loss: 0.6538 - val_loss: 0.6516\n",
      "Epoch 19/1000\n",
      "18/18 - 0s - loss: 0.6478 - val_loss: 0.6453\n",
      "Epoch 20/1000\n",
      "18/18 - 0s - loss: 0.6410 - val_loss: 0.6381\n",
      "Epoch 21/1000\n",
      "18/18 - 0s - loss: 0.6334 - val_loss: 0.6302\n",
      "Epoch 22/1000\n",
      "18/18 - 0s - loss: 0.6249 - val_loss: 0.6215\n",
      "Epoch 23/1000\n",
      "18/18 - 0s - loss: 0.6155 - val_loss: 0.6119\n",
      "Epoch 24/1000\n",
      "18/18 - 0s - loss: 0.6051 - val_loss: 0.6015\n",
      "Epoch 25/1000\n",
      "18/18 - 0s - loss: 0.5937 - val_loss: 0.5899\n",
      "Epoch 26/1000\n",
      "18/18 - 0s - loss: 0.5812 - val_loss: 0.5772\n",
      "Epoch 27/1000\n",
      "18/18 - 0s - loss: 0.5676 - val_loss: 0.5633\n",
      "Epoch 28/1000\n",
      "18/18 - 0s - loss: 0.5529 - val_loss: 0.5483\n",
      "Epoch 29/1000\n",
      "18/18 - 0s - loss: 0.5371 - val_loss: 0.5324\n",
      "Epoch 30/1000\n",
      "18/18 - 0s - loss: 0.5202 - val_loss: 0.5155\n",
      "Epoch 31/1000\n",
      "18/18 - 0s - loss: 0.5024 - val_loss: 0.4976\n",
      "Epoch 32/1000\n",
      "18/18 - 0s - loss: 0.4836 - val_loss: 0.4788\n",
      "Epoch 33/1000\n",
      "18/18 - 0s - loss: 0.4641 - val_loss: 0.4593\n",
      "Epoch 34/1000\n",
      "18/18 - 0s - loss: 0.4438 - val_loss: 0.4390\n",
      "Epoch 35/1000\n",
      "18/18 - 0s - loss: 0.4231 - val_loss: 0.4183\n",
      "Epoch 36/1000\n",
      "18/18 - 0s - loss: 0.4020 - val_loss: 0.3974\n",
      "Epoch 37/1000\n",
      "18/18 - 0s - loss: 0.3809 - val_loss: 0.3764\n",
      "Epoch 38/1000\n",
      "18/18 - 0s - loss: 0.3597 - val_loss: 0.3555\n",
      "Epoch 39/1000\n",
      "18/18 - 0s - loss: 0.3389 - val_loss: 0.3350\n",
      "Epoch 40/1000\n",
      "18/18 - 0s - loss: 0.3185 - val_loss: 0.3149\n",
      "Epoch 41/1000\n",
      "18/18 - 0s - loss: 0.2988 - val_loss: 0.2954\n",
      "Epoch 42/1000\n",
      "18/18 - 0s - loss: 0.2799 - val_loss: 0.2768\n",
      "Epoch 43/1000\n",
      "18/18 - 0s - loss: 0.2618 - val_loss: 0.2590\n",
      "Epoch 44/1000\n",
      "18/18 - 0s - loss: 0.2448 - val_loss: 0.2423\n",
      "Epoch 45/1000\n",
      "18/18 - 0s - loss: 0.2288 - val_loss: 0.2267\n",
      "Epoch 46/1000\n",
      "18/18 - 0s - loss: 0.2139 - val_loss: 0.2121\n",
      "Epoch 47/1000\n",
      "18/18 - 0s - loss: 0.2001 - val_loss: 0.1986\n",
      "Epoch 48/1000\n",
      "18/18 - 0s - loss: 0.1874 - val_loss: 0.1861\n",
      "Epoch 49/1000\n",
      "18/18 - 0s - loss: 0.1757 - val_loss: 0.1746\n",
      "Epoch 50/1000\n",
      "18/18 - 0s - loss: 0.1650 - val_loss: 0.1641\n",
      "Epoch 51/1000\n",
      "18/18 - 0s - loss: 0.1552 - val_loss: 0.1545\n",
      "Epoch 52/1000\n",
      "18/18 - 0s - loss: 0.1463 - val_loss: 0.1458\n",
      "Epoch 53/1000\n",
      "18/18 - 0s - loss: 0.1381 - val_loss: 0.1378\n",
      "Epoch 54/1000\n",
      "18/18 - 0s - loss: 0.1307 - val_loss: 0.1305\n",
      "Epoch 55/1000\n",
      "18/18 - 0s - loss: 0.1239 - val_loss: 0.1238\n",
      "Epoch 56/1000\n",
      "18/18 - 0s - loss: 0.1177 - val_loss: 0.1177\n",
      "Epoch 57/1000\n",
      "18/18 - 0s - loss: 0.1120 - val_loss: 0.1120\n",
      "Epoch 58/1000\n",
      "18/18 - 0s - loss: 0.1068 - val_loss: 0.1069\n",
      "Epoch 59/1000\n",
      "18/18 - 0s - loss: 0.1021 - val_loss: 0.1022\n",
      "Epoch 60/1000\n",
      "18/18 - 0s - loss: 0.0977 - val_loss: 0.0978\n",
      "Epoch 61/1000\n",
      "18/18 - 0s - loss: 0.0936 - val_loss: 0.0938\n",
      "Epoch 62/1000\n",
      "18/18 - 0s - loss: 0.0899 - val_loss: 0.0901\n",
      "Epoch 63/1000\n",
      "18/18 - 0s - loss: 0.0864 - val_loss: 0.0866\n",
      "Epoch 64/1000\n",
      "18/18 - 0s - loss: 0.0832 - val_loss: 0.0834\n",
      "Epoch 65/1000\n",
      "18/18 - 0s - loss: 0.0802 - val_loss: 0.0804\n",
      "Epoch 66/1000\n",
      "18/18 - 0s - loss: 0.0774 - val_loss: 0.0777\n",
      "Epoch 67/1000\n",
      "18/18 - 0s - loss: 0.0748 - val_loss: 0.0751\n",
      "Epoch 68/1000\n",
      "18/18 - 0s - loss: 0.0724 - val_loss: 0.0726\n",
      "Epoch 69/1000\n",
      "18/18 - 0s - loss: 0.0701 - val_loss: 0.0704\n",
      "Epoch 70/1000\n",
      "18/18 - 0s - loss: 0.0680 - val_loss: 0.0682\n",
      "Epoch 71/1000\n",
      "18/18 - 0s - loss: 0.0660 - val_loss: 0.0662\n",
      "Epoch 72/1000\n",
      "18/18 - 0s - loss: 0.0640 - val_loss: 0.0643\n",
      "Epoch 73/1000\n",
      "18/18 - 0s - loss: 0.0622 - val_loss: 0.0625\n",
      "Epoch 74/1000\n",
      "18/18 - 0s - loss: 0.0605 - val_loss: 0.0608\n",
      "Epoch 75/1000\n",
      "18/18 - 0s - loss: 0.0589 - val_loss: 0.0592\n",
      "Epoch 76/1000\n",
      "18/18 - 0s - loss: 0.0574 - val_loss: 0.0577\n",
      "Epoch 77/1000\n",
      "18/18 - 0s - loss: 0.0559 - val_loss: 0.0562\n",
      "Epoch 78/1000\n",
      "18/18 - 0s - loss: 0.0546 - val_loss: 0.0548\n",
      "Epoch 79/1000\n",
      "18/18 - 0s - loss: 0.0532 - val_loss: 0.0535\n",
      "Epoch 80/1000\n",
      "18/18 - 0s - loss: 0.0520 - val_loss: 0.0523\n",
      "Epoch 81/1000\n",
      "18/18 - 0s - loss: 0.0508 - val_loss: 0.0511\n",
      "Epoch 82/1000\n",
      "18/18 - 0s - loss: 0.0496 - val_loss: 0.0499\n",
      "Epoch 83/1000\n",
      "18/18 - 0s - loss: 0.0485 - val_loss: 0.0488\n",
      "Epoch 84/1000\n",
      "18/18 - 0s - loss: 0.0475 - val_loss: 0.0478\n",
      "Epoch 85/1000\n",
      "18/18 - 0s - loss: 0.0465 - val_loss: 0.0468\n",
      "Epoch 86/1000\n",
      "18/18 - 0s - loss: 0.0455 - val_loss: 0.0458\n",
      "Epoch 87/1000\n",
      "18/18 - 0s - loss: 0.0446 - val_loss: 0.0449\n",
      "Epoch 88/1000\n",
      "18/18 - 0s - loss: 0.0437 - val_loss: 0.0440\n",
      "Epoch 89/1000\n",
      "18/18 - 0s - loss: 0.0428 - val_loss: 0.0431\n",
      "Epoch 90/1000\n",
      "18/18 - 0s - loss: 0.0420 - val_loss: 0.0423\n",
      "Epoch 91/1000\n",
      "18/18 - 0s - loss: 0.0412 - val_loss: 0.0415\n",
      "Epoch 92/1000\n",
      "18/18 - 0s - loss: 0.0404 - val_loss: 0.0407\n",
      "Epoch 93/1000\n",
      "18/18 - 0s - loss: 0.0397 - val_loss: 0.0400\n",
      "Epoch 94/1000\n",
      "18/18 - 0s - loss: 0.0390 - val_loss: 0.0393\n",
      "Epoch 95/1000\n",
      "18/18 - 0s - loss: 0.0383 - val_loss: 0.0386\n",
      "Epoch 96/1000\n",
      "18/18 - 0s - loss: 0.0376 - val_loss: 0.0379\n",
      "Epoch 97/1000\n",
      "18/18 - 0s - loss: 0.0369 - val_loss: 0.0372\n",
      "Epoch 98/1000\n",
      "18/18 - 0s - loss: 0.0363 - val_loss: 0.0366\n",
      "Epoch 99/1000\n",
      "18/18 - 0s - loss: 0.0357 - val_loss: 0.0360\n",
      "Epoch 100/1000\n",
      "18/18 - 0s - loss: 0.0351 - val_loss: 0.0354\n",
      "Epoch 101/1000\n",
      "18/18 - 0s - loss: 0.0345 - val_loss: 0.0348\n",
      "Epoch 102/1000\n",
      "18/18 - 0s - loss: 0.0340 - val_loss: 0.0343\n",
      "Epoch 103/1000\n",
      "18/18 - 0s - loss: 0.0334 - val_loss: 0.0337\n",
      "Epoch 104/1000\n",
      "18/18 - 0s - loss: 0.0329 - val_loss: 0.0332\n",
      "Epoch 105/1000\n",
      "18/18 - 0s - loss: 0.0324 - val_loss: 0.0327\n",
      "Epoch 106/1000\n",
      "18/18 - 0s - loss: 0.0319 - val_loss: 0.0322\n",
      "Epoch 107/1000\n",
      "18/18 - 0s - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 108/1000\n",
      "18/18 - 0s - loss: 0.0310 - val_loss: 0.0312\n",
      "Epoch 109/1000\n",
      "18/18 - 0s - loss: 0.0305 - val_loss: 0.0308\n",
      "Epoch 110/1000\n",
      "18/18 - 0s - loss: 0.0301 - val_loss: 0.0303\n",
      "Epoch 111/1000\n",
      "18/18 - 0s - loss: 0.0296 - val_loss: 0.0299\n",
      "Epoch 112/1000\n",
      "18/18 - 0s - loss: 0.0292 - val_loss: 0.0295\n",
      "Epoch 113/1000\n",
      "18/18 - 0s - loss: 0.0288 - val_loss: 0.0291\n",
      "Epoch 114/1000\n",
      "18/18 - 0s - loss: 0.0284 - val_loss: 0.0287\n",
      "Epoch 115/1000\n",
      "18/18 - 0s - loss: 0.0280 - val_loss: 0.0283\n",
      "Epoch 116/1000\n",
      "18/18 - 0s - loss: 0.0276 - val_loss: 0.0279\n",
      "Epoch 117/1000\n",
      "18/18 - 0s - loss: 0.0272 - val_loss: 0.0275\n",
      "Epoch 118/1000\n",
      "18/18 - 0s - loss: 0.0269 - val_loss: 0.0271\n",
      "Epoch 119/1000\n",
      "18/18 - 0s - loss: 0.0265 - val_loss: 0.0268\n",
      "Epoch 120/1000\n",
      "18/18 - 0s - loss: 0.0262 - val_loss: 0.0264\n",
      "Epoch 121/1000\n",
      "18/18 - 0s - loss: 0.0258 - val_loss: 0.0261\n",
      "Epoch 122/1000\n",
      "18/18 - 0s - loss: 0.0255 - val_loss: 0.0257\n",
      "Epoch 123/1000\n",
      "18/18 - 0s - loss: 0.0251 - val_loss: 0.0254\n",
      "Epoch 124/1000\n",
      "18/18 - 0s - loss: 0.0248 - val_loss: 0.0251\n",
      "Epoch 125/1000\n",
      "18/18 - 0s - loss: 0.0245 - val_loss: 0.0248\n",
      "Epoch 126/1000\n",
      "18/18 - 0s - loss: 0.0242 - val_loss: 0.0245\n",
      "Epoch 127/1000\n",
      "18/18 - 0s - loss: 0.0239 - val_loss: 0.0242\n",
      "Epoch 128/1000\n",
      "18/18 - 0s - loss: 0.0236 - val_loss: 0.0239\n",
      "Epoch 129/1000\n",
      "18/18 - 0s - loss: 0.0233 - val_loss: 0.0236\n",
      "Epoch 130/1000\n",
      "18/18 - 0s - loss: 0.0230 - val_loss: 0.0233\n",
      "Epoch 131/1000\n",
      "18/18 - 0s - loss: 0.0228 - val_loss: 0.0230\n",
      "Epoch 132/1000\n",
      "18/18 - 0s - loss: 0.0225 - val_loss: 0.0227\n",
      "Epoch 133/1000\n",
      "18/18 - 0s - loss: 0.0222 - val_loss: 0.0225\n",
      "Epoch 134/1000\n",
      "18/18 - 0s - loss: 0.0220 - val_loss: 0.0222\n",
      "Epoch 135/1000\n",
      "18/18 - 0s - loss: 0.0217 - val_loss: 0.0220\n",
      "Epoch 136/1000\n",
      "18/18 - 0s - loss: 0.0215 - val_loss: 0.0217\n",
      "Epoch 137/1000\n",
      "18/18 - 0s - loss: 0.0212 - val_loss: 0.0215\n",
      "Epoch 138/1000\n",
      "18/18 - 0s - loss: 0.0210 - val_loss: 0.0212\n",
      "Epoch 139/1000\n",
      "18/18 - 0s - loss: 0.0207 - val_loss: 0.0210\n",
      "Epoch 140/1000\n",
      "18/18 - 0s - loss: 0.0205 - val_loss: 0.0207\n",
      "Epoch 141/1000\n",
      "18/18 - 0s - loss: 0.0203 - val_loss: 0.0205\n",
      "Epoch 142/1000\n",
      "18/18 - 0s - loss: 0.0201 - val_loss: 0.0203\n",
      "Epoch 143/1000\n",
      "18/18 - 0s - loss: 0.0198 - val_loss: 0.0201\n",
      "Epoch 144/1000\n",
      "18/18 - 0s - loss: 0.0196 - val_loss: 0.0199\n",
      "Epoch 145/1000\n",
      "18/18 - 0s - loss: 0.0194 - val_loss: 0.0196\n",
      "Epoch 146/1000\n",
      "18/18 - 0s - loss: 0.0192 - val_loss: 0.0194\n",
      "Epoch 147/1000\n",
      "18/18 - 0s - loss: 0.0190 - val_loss: 0.0192\n",
      "Epoch 148/1000\n",
      "18/18 - 0s - loss: 0.0188 - val_loss: 0.0190\n",
      "Epoch 149/1000\n",
      "18/18 - 0s - loss: 0.0186 - val_loss: 0.0188\n",
      "Epoch 150/1000\n",
      "18/18 - 0s - loss: 0.0184 - val_loss: 0.0186\n",
      "Epoch 151/1000\n",
      "18/18 - 0s - loss: 0.0182 - val_loss: 0.0184\n",
      "Epoch 152/1000\n",
      "18/18 - 0s - loss: 0.0180 - val_loss: 0.0182\n",
      "Epoch 153/1000\n",
      "18/18 - 0s - loss: 0.0178 - val_loss: 0.0181\n",
      "Epoch 154/1000\n",
      "18/18 - 0s - loss: 0.0177 - val_loss: 0.0179\n",
      "Epoch 155/1000\n",
      "18/18 - 0s - loss: 0.0175 - val_loss: 0.0177\n",
      "Epoch 00155: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22034a43518>"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Create a test/train split.  25% test\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create neural net\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='tanh'))\n",
    "model.add(Dense(50, input_dim=x.shape[1], kernel_initializer='normal', activation='tanh'))\n",
    "model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='tanh'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Measure accuracy\n",
    "pred = model.predict(x_test)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_eval = np.argmax(y_test,axis=1)\n",
    "score = metrics.accuracy_score(y_eval, pred)\n",
    "print(\"Validation score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26 samples, validate on 9 samples\n",
      "Epoch 1/1000\n",
      "26/26 - 0s - loss: 0.6929 - val_loss: 0.6924\n",
      "Epoch 2/1000\n",
      "26/26 - 0s - loss: 0.6927 - val_loss: 0.6921\n",
      "Epoch 3/1000\n",
      "26/26 - 0s - loss: 0.6924 - val_loss: 0.6918\n",
      "Epoch 4/1000\n",
      "26/26 - 0s - loss: 0.6922 - val_loss: 0.6914\n",
      "Epoch 5/1000\n",
      "26/26 - 0s - loss: 0.6919 - val_loss: 0.6911\n",
      "Epoch 6/1000\n",
      "26/26 - 0s - loss: 0.6916 - val_loss: 0.6907\n",
      "Epoch 7/1000\n",
      "26/26 - 0s - loss: 0.6913 - val_loss: 0.6903\n",
      "Epoch 8/1000\n",
      "26/26 - 0s - loss: 0.6910 - val_loss: 0.6899\n",
      "Epoch 9/1000\n",
      "26/26 - 0s - loss: 0.6907 - val_loss: 0.6895\n",
      "Epoch 10/1000\n",
      "26/26 - 0s - loss: 0.6904 - val_loss: 0.6890\n",
      "Epoch 11/1000\n",
      "26/26 - 0s - loss: 0.6900 - val_loss: 0.6886\n",
      "Epoch 12/1000\n",
      "26/26 - 0s - loss: 0.6896 - val_loss: 0.6880\n",
      "Epoch 13/1000\n",
      "26/26 - 0s - loss: 0.6892 - val_loss: 0.6875\n",
      "Epoch 14/1000\n",
      "26/26 - 0s - loss: 0.6887 - val_loss: 0.6869\n",
      "Epoch 15/1000\n",
      "26/26 - 0s - loss: 0.6882 - val_loss: 0.6862\n",
      "Epoch 16/1000\n",
      "26/26 - 0s - loss: 0.6877 - val_loss: 0.6855\n",
      "Epoch 17/1000\n",
      "26/26 - 0s - loss: 0.6871 - val_loss: 0.6848\n",
      "Epoch 18/1000\n",
      "26/26 - 0s - loss: 0.6865 - val_loss: 0.6840\n",
      "Epoch 19/1000\n",
      "26/26 - 0s - loss: 0.6858 - val_loss: 0.6831\n",
      "Epoch 20/1000\n",
      "26/26 - 0s - loss: 0.6850 - val_loss: 0.6821\n",
      "Epoch 21/1000\n",
      "26/26 - 0s - loss: 0.6842 - val_loss: 0.6811\n",
      "Epoch 22/1000\n",
      "26/26 - 0s - loss: 0.6833 - val_loss: 0.6799\n",
      "Epoch 23/1000\n",
      "26/26 - 0s - loss: 0.6823 - val_loss: 0.6787\n",
      "Epoch 24/1000\n",
      "26/26 - 0s - loss: 0.6812 - val_loss: 0.6774\n",
      "Epoch 25/1000\n",
      "26/26 - 0s - loss: 0.6800 - val_loss: 0.6759\n",
      "Epoch 26/1000\n",
      "26/26 - 0s - loss: 0.6788 - val_loss: 0.6743\n",
      "Epoch 27/1000\n",
      "26/26 - 0s - loss: 0.6774 - val_loss: 0.6726\n",
      "Epoch 28/1000\n",
      "26/26 - 0s - loss: 0.6759 - val_loss: 0.6708\n",
      "Epoch 29/1000\n",
      "26/26 - 0s - loss: 0.6742 - val_loss: 0.6688\n",
      "Epoch 30/1000\n",
      "26/26 - 0s - loss: 0.6724 - val_loss: 0.6666\n",
      "Epoch 31/1000\n",
      "26/26 - 0s - loss: 0.6705 - val_loss: 0.6643\n",
      "Epoch 32/1000\n",
      "26/26 - 0s - loss: 0.6684 - val_loss: 0.6618\n",
      "Epoch 33/1000\n",
      "26/26 - 0s - loss: 0.6661 - val_loss: 0.6591\n",
      "Epoch 34/1000\n",
      "26/26 - 0s - loss: 0.6637 - val_loss: 0.6561\n",
      "Epoch 35/1000\n",
      "26/26 - 0s - loss: 0.6610 - val_loss: 0.6530\n",
      "Epoch 36/1000\n",
      "26/26 - 0s - loss: 0.6581 - val_loss: 0.6496\n",
      "Epoch 37/1000\n",
      "26/26 - 0s - loss: 0.6551 - val_loss: 0.6460\n",
      "Epoch 38/1000\n",
      "26/26 - 0s - loss: 0.6518 - val_loss: 0.6422\n",
      "Epoch 39/1000\n",
      "26/26 - 0s - loss: 0.6482 - val_loss: 0.6381\n",
      "Epoch 40/1000\n",
      "26/26 - 0s - loss: 0.6444 - val_loss: 0.6337\n",
      "Epoch 41/1000\n",
      "26/26 - 0s - loss: 0.6404 - val_loss: 0.6291\n",
      "Epoch 42/1000\n",
      "26/26 - 0s - loss: 0.6361 - val_loss: 0.6242\n",
      "Epoch 43/1000\n",
      "26/26 - 0s - loss: 0.6315 - val_loss: 0.6190\n",
      "Epoch 44/1000\n",
      "26/26 - 0s - loss: 0.6266 - val_loss: 0.6135\n",
      "Epoch 45/1000\n",
      "26/26 - 0s - loss: 0.6215 - val_loss: 0.6077\n",
      "Epoch 46/1000\n",
      "26/26 - 0s - loss: 0.6160 - val_loss: 0.6017\n",
      "Epoch 47/1000\n",
      "26/26 - 0s - loss: 0.6103 - val_loss: 0.5953\n",
      "Epoch 48/1000\n",
      "26/26 - 0s - loss: 0.6043 - val_loss: 0.5887\n",
      "Epoch 49/1000\n",
      "26/26 - 0s - loss: 0.5980 - val_loss: 0.5819\n",
      "Epoch 50/1000\n",
      "26/26 - 0s - loss: 0.5914 - val_loss: 0.5747\n",
      "Epoch 51/1000\n",
      "26/26 - 0s - loss: 0.5846 - val_loss: 0.5673\n",
      "Epoch 52/1000\n",
      "26/26 - 0s - loss: 0.5774 - val_loss: 0.5597\n",
      "Epoch 53/1000\n",
      "26/26 - 0s - loss: 0.5700 - val_loss: 0.5518\n",
      "Epoch 54/1000\n",
      "26/26 - 0s - loss: 0.5624 - val_loss: 0.5437\n",
      "Epoch 55/1000\n",
      "26/26 - 0s - loss: 0.5545 - val_loss: 0.5354\n",
      "Epoch 56/1000\n",
      "26/26 - 0s - loss: 0.5464 - val_loss: 0.5269\n",
      "Epoch 57/1000\n",
      "26/26 - 0s - loss: 0.5381 - val_loss: 0.5183\n",
      "Epoch 58/1000\n",
      "26/26 - 0s - loss: 0.5296 - val_loss: 0.5095\n",
      "Epoch 59/1000\n",
      "26/26 - 0s - loss: 0.5209 - val_loss: 0.5006\n",
      "Epoch 60/1000\n",
      "26/26 - 0s - loss: 0.5121 - val_loss: 0.4916\n",
      "Epoch 61/1000\n",
      "26/26 - 0s - loss: 0.5031 - val_loss: 0.4825\n",
      "Epoch 62/1000\n",
      "26/26 - 0s - loss: 0.4941 - val_loss: 0.4733\n",
      "Epoch 63/1000\n",
      "26/26 - 0s - loss: 0.4849 - val_loss: 0.4642\n",
      "Epoch 64/1000\n",
      "26/26 - 0s - loss: 0.4757 - val_loss: 0.4550\n",
      "Epoch 65/1000\n",
      "26/26 - 0s - loss: 0.4664 - val_loss: 0.4458\n",
      "Epoch 66/1000\n",
      "26/26 - 0s - loss: 0.4572 - val_loss: 0.4366\n",
      "Epoch 67/1000\n",
      "26/26 - 0s - loss: 0.4479 - val_loss: 0.4275\n",
      "Epoch 68/1000\n",
      "26/26 - 0s - loss: 0.4386 - val_loss: 0.4184\n",
      "Epoch 69/1000\n",
      "26/26 - 0s - loss: 0.4293 - val_loss: 0.4094\n",
      "Epoch 70/1000\n",
      "26/26 - 0s - loss: 0.4201 - val_loss: 0.4005\n",
      "Epoch 71/1000\n",
      "26/26 - 0s - loss: 0.4110 - val_loss: 0.3917\n",
      "Epoch 72/1000\n",
      "26/26 - 0s - loss: 0.4020 - val_loss: 0.3830\n",
      "Epoch 73/1000\n",
      "26/26 - 0s - loss: 0.3931 - val_loss: 0.3745\n",
      "Epoch 74/1000\n",
      "26/26 - 0s - loss: 0.3842 - val_loss: 0.3661\n",
      "Epoch 75/1000\n",
      "26/26 - 0s - loss: 0.3755 - val_loss: 0.3578\n",
      "Epoch 76/1000\n",
      "26/26 - 0s - loss: 0.3670 - val_loss: 0.3497\n",
      "Epoch 77/1000\n",
      "26/26 - 0s - loss: 0.3586 - val_loss: 0.3417\n",
      "Epoch 78/1000\n",
      "26/26 - 0s - loss: 0.3503 - val_loss: 0.3339\n",
      "Epoch 79/1000\n",
      "26/26 - 0s - loss: 0.3422 - val_loss: 0.3262\n",
      "Epoch 80/1000\n",
      "26/26 - 0s - loss: 0.3343 - val_loss: 0.3188\n",
      "Epoch 81/1000\n",
      "26/26 - 0s - loss: 0.3266 - val_loss: 0.3115\n",
      "Epoch 82/1000\n",
      "26/26 - 0s - loss: 0.3190 - val_loss: 0.3043\n",
      "Epoch 83/1000\n",
      "26/26 - 0s - loss: 0.3116 - val_loss: 0.2974\n",
      "Epoch 84/1000\n",
      "26/26 - 0s - loss: 0.3044 - val_loss: 0.2906\n",
      "Epoch 85/1000\n",
      "26/26 - 0s - loss: 0.2973 - val_loss: 0.2840\n",
      "Epoch 86/1000\n",
      "26/26 - 0s - loss: 0.2904 - val_loss: 0.2775\n",
      "Epoch 87/1000\n",
      "26/26 - 0s - loss: 0.2838 - val_loss: 0.2713\n",
      "Epoch 88/1000\n",
      "26/26 - 0s - loss: 0.2772 - val_loss: 0.2651\n",
      "Epoch 89/1000\n",
      "26/26 - 0s - loss: 0.2709 - val_loss: 0.2592\n",
      "Epoch 90/1000\n",
      "26/26 - 0s - loss: 0.2647 - val_loss: 0.2534\n",
      "Epoch 91/1000\n",
      "26/26 - 0s - loss: 0.2588 - val_loss: 0.2478\n",
      "Epoch 92/1000\n",
      "26/26 - 0s - loss: 0.2529 - val_loss: 0.2423\n",
      "Epoch 93/1000\n",
      "26/26 - 0s - loss: 0.2473 - val_loss: 0.2370\n",
      "Epoch 94/1000\n",
      "26/26 - 0s - loss: 0.2418 - val_loss: 0.2318\n",
      "Epoch 95/1000\n",
      "26/26 - 0s - loss: 0.2364 - val_loss: 0.2268\n",
      "Epoch 96/1000\n",
      "26/26 - 0s - loss: 0.2312 - val_loss: 0.2219\n",
      "Epoch 97/1000\n",
      "26/26 - 0s - loss: 0.2262 - val_loss: 0.2172\n",
      "Epoch 98/1000\n",
      "26/26 - 0s - loss: 0.2213 - val_loss: 0.2125\n",
      "Epoch 99/1000\n",
      "26/26 - 0s - loss: 0.2165 - val_loss: 0.2081\n",
      "Epoch 100/1000\n",
      "26/26 - 0s - loss: 0.2119 - val_loss: 0.2037\n",
      "Epoch 101/1000\n",
      "26/26 - 0s - loss: 0.2074 - val_loss: 0.1995\n",
      "Epoch 102/1000\n",
      "26/26 - 0s - loss: 0.2030 - val_loss: 0.1953\n",
      "Epoch 103/1000\n",
      "26/26 - 0s - loss: 0.1988 - val_loss: 0.1913\n",
      "Epoch 104/1000\n",
      "26/26 - 0s - loss: 0.1947 - val_loss: 0.1875\n",
      "Epoch 105/1000\n",
      "26/26 - 0s - loss: 0.1907 - val_loss: 0.1837\n",
      "Epoch 106/1000\n",
      "26/26 - 0s - loss: 0.1868 - val_loss: 0.1800\n",
      "Epoch 107/1000\n",
      "26/26 - 0s - loss: 0.1830 - val_loss: 0.1764\n",
      "Epoch 108/1000\n",
      "26/26 - 0s - loss: 0.1794 - val_loss: 0.1730\n",
      "Epoch 109/1000\n",
      "26/26 - 0s - loss: 0.1758 - val_loss: 0.1696\n",
      "Epoch 110/1000\n",
      "26/26 - 0s - loss: 0.1723 - val_loss: 0.1663\n",
      "Epoch 111/1000\n",
      "26/26 - 0s - loss: 0.1690 - val_loss: 0.1631\n",
      "Epoch 112/1000\n",
      "26/26 - 0s - loss: 0.1657 - val_loss: 0.1600\n",
      "Epoch 113/1000\n",
      "26/26 - 0s - loss: 0.1625 - val_loss: 0.1570\n",
      "Epoch 114/1000\n",
      "26/26 - 0s - loss: 0.1594 - val_loss: 0.1540\n",
      "Epoch 115/1000\n",
      "26/26 - 0s - loss: 0.1564 - val_loss: 0.1512\n",
      "Epoch 116/1000\n",
      "26/26 - 0s - loss: 0.1535 - val_loss: 0.1484\n",
      "Epoch 117/1000\n",
      "26/26 - 0s - loss: 0.1506 - val_loss: 0.1457\n",
      "Epoch 118/1000\n",
      "26/26 - 0s - loss: 0.1479 - val_loss: 0.1430\n",
      "Epoch 119/1000\n",
      "26/26 - 0s - loss: 0.1452 - val_loss: 0.1405\n",
      "Epoch 120/1000\n",
      "26/26 - 0s - loss: 0.1425 - val_loss: 0.1380\n",
      "Epoch 121/1000\n",
      "26/26 - 0s - loss: 0.1400 - val_loss: 0.1355\n",
      "Epoch 122/1000\n",
      "26/26 - 0s - loss: 0.1375 - val_loss: 0.1332\n",
      "Epoch 123/1000\n",
      "26/26 - 0s - loss: 0.1351 - val_loss: 0.1308\n",
      "Epoch 124/1000\n",
      "26/26 - 0s - loss: 0.1327 - val_loss: 0.1286\n",
      "Epoch 125/1000\n",
      "26/26 - 0s - loss: 0.1304 - val_loss: 0.1264\n",
      "Epoch 126/1000\n",
      "26/26 - 0s - loss: 0.1282 - val_loss: 0.1242\n",
      "Epoch 127/1000\n",
      "26/26 - 0s - loss: 0.1260 - val_loss: 0.1222\n",
      "Epoch 128/1000\n",
      "26/26 - 0s - loss: 0.1238 - val_loss: 0.1201\n",
      "Epoch 129/1000\n",
      "26/26 - 0s - loss: 0.1218 - val_loss: 0.1181\n",
      "Epoch 130/1000\n",
      "26/26 - 0s - loss: 0.1197 - val_loss: 0.1162\n",
      "Epoch 131/1000\n",
      "26/26 - 0s - loss: 0.1178 - val_loss: 0.1143\n",
      "Epoch 132/1000\n",
      "26/26 - 0s - loss: 0.1158 - val_loss: 0.1124\n",
      "Epoch 133/1000\n",
      "26/26 - 0s - loss: 0.1140 - val_loss: 0.1106\n",
      "Epoch 134/1000\n",
      "26/26 - 0s - loss: 0.1121 - val_loss: 0.1089\n",
      "Epoch 135/1000\n",
      "26/26 - 0s - loss: 0.1103 - val_loss: 0.1072\n",
      "Epoch 136/1000\n",
      "26/26 - 0s - loss: 0.1086 - val_loss: 0.1055\n",
      "Epoch 137/1000\n",
      "26/26 - 0s - loss: 0.1069 - val_loss: 0.1038\n",
      "Epoch 138/1000\n",
      "26/26 - 0s - loss: 0.1052 - val_loss: 0.1022\n",
      "Epoch 139/1000\n",
      "26/26 - 0s - loss: 0.1036 - val_loss: 0.1007\n",
      "Epoch 140/1000\n",
      "26/26 - 0s - loss: 0.1020 - val_loss: 0.0992\n",
      "Epoch 141/1000\n",
      "26/26 - 0s - loss: 0.1004 - val_loss: 0.0977\n",
      "Epoch 142/1000\n",
      "26/26 - 0s - loss: 0.0989 - val_loss: 0.0962\n",
      "Epoch 143/1000\n",
      "26/26 - 0s - loss: 0.0974 - val_loss: 0.0948\n",
      "Epoch 144/1000\n",
      "26/26 - 0s - loss: 0.0960 - val_loss: 0.0934\n",
      "Epoch 145/1000\n",
      "26/26 - 0s - loss: 0.0945 - val_loss: 0.0920\n",
      "Epoch 146/1000\n",
      "26/26 - 0s - loss: 0.0932 - val_loss: 0.0907\n",
      "Epoch 147/1000\n",
      "26/26 - 0s - loss: 0.0918 - val_loss: 0.0894\n",
      "Epoch 148/1000\n",
      "26/26 - 0s - loss: 0.0905 - val_loss: 0.0881\n",
      "Epoch 149/1000\n",
      "26/26 - 0s - loss: 0.0892 - val_loss: 0.0868\n",
      "Epoch 150/1000\n",
      "26/26 - 0s - loss: 0.0879 - val_loss: 0.0856\n",
      "Epoch 151/1000\n",
      "26/26 - 0s - loss: 0.0867 - val_loss: 0.0844\n",
      "Epoch 152/1000\n",
      "26/26 - 0s - loss: 0.0854 - val_loss: 0.0832\n",
      "Epoch 153/1000\n",
      "26/26 - 0s - loss: 0.0842 - val_loss: 0.0821\n",
      "Epoch 154/1000\n",
      "26/26 - 0s - loss: 0.0831 - val_loss: 0.0810\n",
      "Epoch 155/1000\n",
      "26/26 - 0s - loss: 0.0819 - val_loss: 0.0799\n",
      "Epoch 156/1000\n",
      "26/26 - 0s - loss: 0.0808 - val_loss: 0.0788\n",
      "Epoch 157/1000\n",
      "26/26 - 0s - loss: 0.0797 - val_loss: 0.0777\n",
      "Epoch 158/1000\n",
      "26/26 - 0s - loss: 0.0786 - val_loss: 0.0767\n",
      "Epoch 159/1000\n",
      "26/26 - 0s - loss: 0.0776 - val_loss: 0.0757\n",
      "Epoch 160/1000\n",
      "26/26 - 0s - loss: 0.0766 - val_loss: 0.0747\n",
      "Epoch 161/1000\n",
      "26/26 - 0s - loss: 0.0755 - val_loss: 0.0737\n",
      "Epoch 162/1000\n",
      "26/26 - 0s - loss: 0.0746 - val_loss: 0.0727\n",
      "Epoch 163/1000\n",
      "26/26 - 0s - loss: 0.0736 - val_loss: 0.0718\n",
      "Epoch 164/1000\n",
      "26/26 - 0s - loss: 0.0726 - val_loss: 0.0709\n",
      "Epoch 165/1000\n",
      "26/26 - 0s - loss: 0.0717 - val_loss: 0.0700\n",
      "Epoch 166/1000\n",
      "26/26 - 0s - loss: 0.0708 - val_loss: 0.0691\n",
      "Epoch 167/1000\n",
      "26/26 - 0s - loss: 0.0699 - val_loss: 0.0682\n",
      "Epoch 168/1000\n",
      "26/26 - 0s - loss: 0.0690 - val_loss: 0.0674\n",
      "Epoch 169/1000\n",
      "26/26 - 0s - loss: 0.0681 - val_loss: 0.0665\n",
      "Epoch 170/1000\n",
      "26/26 - 0s - loss: 0.0673 - val_loss: 0.0657\n",
      "Epoch 171/1000\n",
      "26/26 - 0s - loss: 0.0665 - val_loss: 0.0649\n",
      "Epoch 172/1000\n",
      "26/26 - 0s - loss: 0.0656 - val_loss: 0.0641\n",
      "Epoch 173/1000\n",
      "26/26 - 0s - loss: 0.0648 - val_loss: 0.0633\n",
      "Epoch 174/1000\n",
      "26/26 - 0s - loss: 0.0641 - val_loss: 0.0626\n",
      "Epoch 175/1000\n",
      "26/26 - 0s - loss: 0.0633 - val_loss: 0.0618\n",
      "Epoch 176/1000\n",
      "26/26 - 0s - loss: 0.0625 - val_loss: 0.0611\n",
      "Epoch 177/1000\n",
      "26/26 - 0s - loss: 0.0618 - val_loss: 0.0604\n",
      "Epoch 178/1000\n",
      "26/26 - 0s - loss: 0.0611 - val_loss: 0.0597\n",
      "Epoch 179/1000\n",
      "26/26 - 0s - loss: 0.0603 - val_loss: 0.0590\n",
      "Epoch 180/1000\n",
      "26/26 - 0s - loss: 0.0596 - val_loss: 0.0583\n",
      "Epoch 181/1000\n",
      "26/26 - 0s - loss: 0.0589 - val_loss: 0.0576\n",
      "Epoch 182/1000\n",
      "26/26 - 0s - loss: 0.0583 - val_loss: 0.0570\n",
      "Epoch 183/1000\n",
      "26/26 - 0s - loss: 0.0576 - val_loss: 0.0563\n",
      "Epoch 184/1000\n",
      "26/26 - 0s - loss: 0.0569 - val_loss: 0.0557\n",
      "Epoch 185/1000\n",
      "26/26 - 0s - loss: 0.0563 - val_loss: 0.0551\n",
      "Epoch 186/1000\n",
      "26/26 - 0s - loss: 0.0556 - val_loss: 0.0544\n",
      "Epoch 187/1000\n",
      "26/26 - 0s - loss: 0.0550 - val_loss: 0.0538\n",
      "Epoch 188/1000\n",
      "26/26 - 0s - loss: 0.0544 - val_loss: 0.0532\n",
      "Epoch 189/1000\n",
      "26/26 - 0s - loss: 0.0538 - val_loss: 0.0526\n",
      "Epoch 190/1000\n",
      "26/26 - 0s - loss: 0.0532 - val_loss: 0.0521\n",
      "Epoch 191/1000\n",
      "26/26 - 0s - loss: 0.0526 - val_loss: 0.0515\n",
      "Epoch 192/1000\n",
      "26/26 - 0s - loss: 0.0521 - val_loss: 0.0510\n",
      "Epoch 193/1000\n",
      "26/26 - 0s - loss: 0.0515 - val_loss: 0.0504\n",
      "Epoch 194/1000\n",
      "26/26 - 0s - loss: 0.0509 - val_loss: 0.0499\n",
      "Epoch 195/1000\n",
      "26/26 - 0s - loss: 0.0504 - val_loss: 0.0493\n",
      "Epoch 196/1000\n",
      "26/26 - 0s - loss: 0.0499 - val_loss: 0.0488\n",
      "Epoch 197/1000\n",
      "26/26 - 0s - loss: 0.0493 - val_loss: 0.0483\n",
      "Epoch 198/1000\n",
      "26/26 - 0s - loss: 0.0488 - val_loss: 0.0478\n",
      "Epoch 199/1000\n",
      "26/26 - 0s - loss: 0.0483 - val_loss: 0.0473\n",
      "Epoch 200/1000\n",
      "26/26 - 0s - loss: 0.0478 - val_loss: 0.0468\n",
      "Epoch 201/1000\n",
      "26/26 - 0s - loss: 0.0473 - val_loss: 0.0463\n",
      "Epoch 202/1000\n",
      "26/26 - 0s - loss: 0.0468 - val_loss: 0.0459\n",
      "Epoch 203/1000\n",
      "26/26 - 0s - loss: 0.0463 - val_loss: 0.0454\n",
      "Epoch 204/1000\n",
      "26/26 - 0s - loss: 0.0459 - val_loss: 0.0449\n",
      "Epoch 205/1000\n",
      "26/26 - 0s - loss: 0.0454 - val_loss: 0.0445\n",
      "Epoch 206/1000\n",
      "26/26 - 0s - loss: 0.0449 - val_loss: 0.0440\n",
      "Epoch 207/1000\n",
      "26/26 - 0s - loss: 0.0445 - val_loss: 0.0436\n",
      "Epoch 208/1000\n",
      "26/26 - 0s - loss: 0.0440 - val_loss: 0.0432\n",
      "Epoch 209/1000\n",
      "26/26 - 0s - loss: 0.0436 - val_loss: 0.0427\n",
      "Epoch 210/1000\n",
      "26/26 - 0s - loss: 0.0432 - val_loss: 0.0423\n",
      "Epoch 211/1000\n",
      "26/26 - 0s - loss: 0.0427 - val_loss: 0.0419\n",
      "Epoch 212/1000\n",
      "26/26 - 0s - loss: 0.0423 - val_loss: 0.0415\n",
      "Epoch 213/1000\n",
      "26/26 - 0s - loss: 0.0419 - val_loss: 0.0411\n",
      "Epoch 214/1000\n",
      "26/26 - 0s - loss: 0.0415 - val_loss: 0.0407\n",
      "Epoch 215/1000\n",
      "26/26 - 0s - loss: 0.0411 - val_loss: 0.0403\n",
      "Epoch 216/1000\n",
      "26/26 - 0s - loss: 0.0407 - val_loss: 0.0399\n",
      "Epoch 217/1000\n",
      "26/26 - 0s - loss: 0.0403 - val_loss: 0.0395\n",
      "Epoch 218/1000\n",
      "26/26 - 0s - loss: 0.0399 - val_loss: 0.0392\n",
      "Epoch 219/1000\n",
      "26/26 - 0s - loss: 0.0396 - val_loss: 0.0388\n",
      "Epoch 220/1000\n",
      "26/26 - 0s - loss: 0.0392 - val_loss: 0.0384\n",
      "Epoch 221/1000\n",
      "26/26 - 0s - loss: 0.0388 - val_loss: 0.0381\n",
      "Epoch 222/1000\n",
      "26/26 - 0s - loss: 0.0385 - val_loss: 0.0377\n",
      "Epoch 223/1000\n",
      "26/26 - 0s - loss: 0.0381 - val_loss: 0.0374\n",
      "Epoch 224/1000\n",
      "26/26 - 0s - loss: 0.0377 - val_loss: 0.0370\n",
      "Epoch 225/1000\n",
      "26/26 - 0s - loss: 0.0374 - val_loss: 0.0367\n",
      "Epoch 226/1000\n",
      "26/26 - 0s - loss: 0.0371 - val_loss: 0.0364\n",
      "Epoch 227/1000\n",
      "26/26 - 0s - loss: 0.0367 - val_loss: 0.0360\n",
      "Epoch 228/1000\n",
      "26/26 - 0s - loss: 0.0364 - val_loss: 0.0357\n",
      "Epoch 229/1000\n",
      "26/26 - 0s - loss: 0.0361 - val_loss: 0.0354\n",
      "Epoch 230/1000\n",
      "26/26 - 0s - loss: 0.0357 - val_loss: 0.0351\n",
      "Epoch 231/1000\n",
      "26/26 - 0s - loss: 0.0354 - val_loss: 0.0348\n",
      "Epoch 232/1000\n",
      "26/26 - 0s - loss: 0.0351 - val_loss: 0.0345\n",
      "Epoch 233/1000\n",
      "26/26 - 0s - loss: 0.0348 - val_loss: 0.0342\n",
      "Epoch 234/1000\n",
      "26/26 - 0s - loss: 0.0345 - val_loss: 0.0339\n",
      "Epoch 235/1000\n",
      "26/26 - 0s - loss: 0.0342 - val_loss: 0.0336\n",
      "Epoch 236/1000\n",
      "26/26 - 0s - loss: 0.0339 - val_loss: 0.0333\n",
      "Epoch 237/1000\n",
      "26/26 - 0s - loss: 0.0336 - val_loss: 0.0330\n",
      "Epoch 238/1000\n",
      "26/26 - 0s - loss: 0.0333 - val_loss: 0.0327\n",
      "Epoch 239/1000\n",
      "26/26 - 0s - loss: 0.0330 - val_loss: 0.0324\n",
      "Epoch 240/1000\n",
      "26/26 - 0s - loss: 0.0327 - val_loss: 0.0321\n",
      "Epoch 241/1000\n",
      "26/26 - 0s - loss: 0.0325 - val_loss: 0.0319\n",
      "Epoch 242/1000\n",
      "26/26 - 0s - loss: 0.0322 - val_loss: 0.0316\n",
      "Epoch 243/1000\n",
      "26/26 - 0s - loss: 0.0319 - val_loss: 0.0313\n",
      "Epoch 244/1000\n",
      "26/26 - 0s - loss: 0.0316 - val_loss: 0.0311\n",
      "Epoch 245/1000\n",
      "26/26 - 0s - loss: 0.0314 - val_loss: 0.0308\n",
      "Epoch 246/1000\n",
      "26/26 - 0s - loss: 0.0311 - val_loss: 0.0306\n",
      "Epoch 247/1000\n",
      "26/26 - 0s - loss: 0.0309 - val_loss: 0.0303\n",
      "Epoch 248/1000\n",
      "26/26 - 0s - loss: 0.0306 - val_loss: 0.0301\n",
      "Epoch 249/1000\n",
      "26/26 - 0s - loss: 0.0303 - val_loss: 0.0298\n",
      "Epoch 250/1000\n",
      "26/26 - 0s - loss: 0.0301 - val_loss: 0.0296\n",
      "Epoch 251/1000\n",
      "26/26 - 0s - loss: 0.0298 - val_loss: 0.0293\n",
      "Epoch 252/1000\n",
      "26/26 - 0s - loss: 0.0296 - val_loss: 0.0291\n",
      "Epoch 253/1000\n",
      "26/26 - 0s - loss: 0.0294 - val_loss: 0.0289\n",
      "Epoch 254/1000\n",
      "26/26 - 0s - loss: 0.0291 - val_loss: 0.0286\n",
      "Epoch 255/1000\n",
      "26/26 - 0s - loss: 0.0289 - val_loss: 0.0284\n",
      "Epoch 256/1000\n",
      "26/26 - 0s - loss: 0.0287 - val_loss: 0.0282\n",
      "Epoch 257/1000\n",
      "26/26 - 0s - loss: 0.0284 - val_loss: 0.0279\n",
      "Epoch 258/1000\n",
      "26/26 - 0s - loss: 0.0282 - val_loss: 0.0277\n",
      "Epoch 259/1000\n",
      "26/26 - 0s - loss: 0.0280 - val_loss: 0.0275\n",
      "Epoch 260/1000\n",
      "26/26 - 0s - loss: 0.0278 - val_loss: 0.0273\n",
      "Epoch 261/1000\n",
      "26/26 - 0s - loss: 0.0275 - val_loss: 0.0271\n",
      "Epoch 262/1000\n",
      "26/26 - 0s - loss: 0.0273 - val_loss: 0.0269\n",
      "Epoch 263/1000\n",
      "26/26 - 0s - loss: 0.0271 - val_loss: 0.0267\n",
      "Epoch 264/1000\n",
      "26/26 - 0s - loss: 0.0269 - val_loss: 0.0265\n",
      "Epoch 265/1000\n",
      "26/26 - 0s - loss: 0.0267 - val_loss: 0.0263\n",
      "Epoch 266/1000\n",
      "26/26 - 0s - loss: 0.0265 - val_loss: 0.0260\n",
      "Epoch 267/1000\n",
      "26/26 - 0s - loss: 0.0263 - val_loss: 0.0259\n",
      "Epoch 268/1000\n",
      "26/26 - 0s - loss: 0.0261 - val_loss: 0.0257\n",
      "Epoch 269/1000\n",
      "26/26 - 0s - loss: 0.0259 - val_loss: 0.0255\n",
      "Epoch 270/1000\n",
      "26/26 - 0s - loss: 0.0257 - val_loss: 0.0253\n",
      "Epoch 271/1000\n",
      "26/26 - 0s - loss: 0.0255 - val_loss: 0.0251\n",
      "Epoch 00271: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2203083a2b0>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Create a test/train split.  25% test\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create neural net\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='tanh'))\n",
    "model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='tanh'))\n",
    "model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='tanh'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Measure accuracy\n",
    "pred = model.predict(x_test)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_eval = np.argmax(y_test,axis=1)\n",
    "score = metrics.accuracy_score(y_eval, pred)\n",
    "print(\"Validation score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#Deep NEUral N Application with dataset\n",
    "#1. Packages\n",
    "import numpy as np;\n",
    "import h5py;\n",
    "import matplotlib.pyplot as plt;\n",
    "#from testCases_v3 import *;\n",
    "#from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward;\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0); # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest';\n",
    "plt.rcParams['image.cmap'] = 'gray';\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train's shape: (26, 39)\n",
      "x_test's shape: (9, 39)\n",
      "y_train's shape: (26, 2)\n",
      "y_test's shape: (9, 2)\n",
      "train_x's shape: (39, 26)\n",
      "test_x's shape: (39, 9)\n",
      "train_y's shape: (2, 26)\n",
      "test_y's shape: (2, 9)\n"
     ]
    }
   ],
   "source": [
    "print (\"x_train's shape: \" + str(x_train.shape));\n",
    "print (\"x_test's shape: \" + str(x_test.shape));\n",
    "print (\"y_train's shape: \" + str(y_train.shape));\n",
    "print (\"y_test's shape: \" + str(y_test.shape));\n",
    "\n",
    "train_x= (x_train).T\n",
    "train_y= (y_train).T\n",
    "test_x= (x_test).T\n",
    "test_y= (y_test).T\n",
    "\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape));\n",
    "print (\"test_x's shape: \" + str(test_x.shape));\n",
    "print (\"train_y's shape: \" + str(train_y.shape));\n",
    "print (\"test_y's shape: \" + str(test_y.shape));\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS DEFINING THE MODEL ####\n",
    "n_x = train_x.shape[0];    #number of features\n",
    "n_h = 9;\n",
    "n_y = 2;\n",
    "layers_dims = (n_x, n_h, n_y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADED FUNCTION: two_layer_model\n",
    "\n",
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples) # rows/ features & number of training examples 25,26\n",
    "    Y -- true \"label\" vector (containing 0 if DOS, 1 if NORMAL), of shape (1, number of examples)(1,26)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
    "\n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1);\n",
    "    grads = {};\n",
    "    costs = [];                              # to keep track of the cost\n",
    "    m = X.shape[1];                           # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims;\n",
    "\n",
    "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y);\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"];\n",
    "    b1 = parameters[\"b1\"];\n",
    "    W2 = parameters[\"W2\"];\n",
    "    b2 = parameters[\"b2\"];\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1\". Output: \"A1, cache1, A2, cache2\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, \"relu\");\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, \"sigmoid\");\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Compute cost\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(A2, Y);\n",
    "      #  print(\"cost = \" + str(cost));        ### END CODE HERE ###\n",
    "\n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2));\n",
    "\n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, \"sigmoid\");\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, \"relu\");\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1;\n",
    "        grads['db1'] = db1;\n",
    "        grads['dW2'] = dW2;\n",
    "        grads['db2'] = db2;\n",
    "\n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (approx. 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate);\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"];\n",
    "        b1 = parameters[\"b1\"];\n",
    "        W2 = parameters[\"W2\"];\n",
    "        b2 = parameters[\"b2\"];\n",
    "\n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "           # print (\"Cost after iteration %i: %f\" %(i, cost));\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost);\n",
    "    #    if print_cost and i % 100 == 0:\n",
    "            #print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "     #                   print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "      #  if print_cost and i % 100 == 0:\n",
    "       #     costs.append(cost)\n",
    "\n",
    " #   # plot the cost\n",
    "\n",
    "   #plt.plot(np.squeeze(cost));\n",
    "      #plt.ylabel('cost');\n",
    "#plt.xlabel('iterations (per tens)');\n",
    " #   plt.title(\"Learning rate =\" + str(learning_rate));\n",
    "  #  plt.show();\n",
    "\n",
    "    return parameters;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: [[0.69331464 0.69312096]\n",
      " [0.69297981 0.69317357]]\n",
      "Cost after iteration 100: [[0.69179433 0.69559537]\n",
      " [0.69467769 0.69086814]]\n",
      "Cost after iteration 200: [[0.68766933 0.70135947]\n",
      " [0.69922647 0.68551646]]\n",
      "Cost after iteration 300: [[0.66806792 0.72486203]\n",
      " [0.71995403 0.66332687]]\n",
      "Cost after iteration 400: [[0.58337437 0.83084185]\n",
      " [0.81986085 0.57489204]]\n",
      "Cost after iteration 500: [[0.38176189 1.20087631]\n",
      " [1.18044901 0.37362857]]\n",
      "Cost after iteration 600: [[0.20943342 1.83738283]\n",
      " [1.81335752 0.20549388]]\n",
      "Cost after iteration 700: [[0.1262404  2.443094  ]\n",
      " [2.42085836 0.12444179]]\n",
      "Cost after iteration 800: [[0.08224229 2.93462325]\n",
      " [2.91478716 0.08129174]]\n",
      "Cost after iteration 900: [[0.05590308 3.33411261]\n",
      " [3.31625823 0.05533794]]\n",
      "Cost after iteration 1000: [[0.03974819 3.66375681]\n",
      " [3.64745418 0.03938279]]\n",
      "Cost after iteration 1100: [[0.02964279 3.93954569]\n",
      " [3.92446539 0.02939049]]\n",
      "Cost after iteration 1200: [[0.02306721 4.17372454]\n",
      " [4.1596243  0.02288352]]\n",
      "Cost after iteration 1300: [[0.01858672 4.37566726]\n",
      " [4.3623687  0.01844721]]\n",
      "Cost after iteration 1400: [[0.01539847 4.55238489]\n",
      " [4.53975354 0.01528887]]\n",
      "Cost after iteration 1500: [[0.01304381 4.70900014]\n",
      " [4.69693388 0.01295534]]\n",
      "Cost after iteration 1600: [[0.01124947 4.84931605]\n",
      " [4.83773533 0.01117644]]\n",
      "Cost after iteration 1700: [[0.00984585 4.97623834]\n",
      " [4.96508018 0.00978445]]\n",
      "Cost after iteration 1800: [[0.00872322 5.09198838]\n",
      " [5.08120183 0.00867081]]\n",
      "Cost after iteration 1900: [[0.00780852 5.19829366]\n",
      " [5.18783698 0.0077632 ]]\n",
      "Cost after iteration 2000: [[0.00705111 5.29650078]\n",
      " [5.28633963 0.00701148]]\n",
      "Cost after iteration 2100: [[0.00641531 5.38772063]\n",
      " [5.37782614 0.00638032]]\n",
      "Cost after iteration 2200: [[0.00587507 5.47285987]\n",
      " [5.46320738 0.00584393]]\n",
      "Cost after iteration 2300: [[5.41121003e-03 5.55264616e+00]\n",
      " [5.54321464e+00 5.38328190e-03]]\n",
      "Cost after iteration 2400: [[5.00920297e-03 5.62769126e+00]\n",
      " [5.61846251e+00 4.98399487e-03]]\n",
      "Cost after iteration 2500: [[4.65792681e-03 5.69850759e+00]\n",
      " [5.68946590e+00 4.63504206e-03]]\n",
      "Cost after iteration 2600: [[4.34864861e-03 5.76554018e+00]\n",
      " [5.75667157e+00 4.32776483e-03]]\n",
      "Cost after iteration 2700: [[4.07456274e-03 5.82916030e+00]\n",
      " [5.82045264e+00 4.05541625e-03]]\n",
      "Cost after iteration 2800: [[3.83018174e-03 5.88969513e+00]\n",
      " [5.88113748e+00 3.81255374e-03]]\n",
      "Cost after iteration 2900: [[3.61111611e-03 5.94741429e+00]\n",
      " [5.93899700e+00 3.59482371e-03]]\n",
      "Cost after iteration 3000: [[3.41375160e-03 6.00256670e+00]\n",
      " [5.99428111e+00 3.39864088e-03]]\n",
      "Cost after iteration 3100: [[3.23513061e-03 6.05536481e+00]\n",
      " [6.04720307e+00 3.22107071e-03]]\n",
      "Cost after iteration 3200: [[3.07279031e-03 6.10600170e+00]\n",
      " [6.09795669e+00 3.05966959e-03]]\n",
      "Cost after iteration 3300: [[2.92469355e-03 6.15463246e+00]\n",
      " [6.14669780e+00 2.91241570e-03]]\n",
      "Cost after iteration 3400: [[2.78908870e-03 6.20141702e+00]\n",
      " [6.19358686e+00 2.77757067e-03]]\n",
      "Cost after iteration 3500: [[2.66451980e-03 6.24648442e+00]\n",
      " [6.23875339e+00 2.65368928e-03]]\n",
      "Cost after iteration 3600: [[2.54973737e-03 6.28995375e+00]\n",
      " [6.28231696e+00 2.53953115e-03]]\n",
      "Cost after iteration 3700: [[2.44368475e-03 6.33192839e+00]\n",
      " [6.32438132e+00 2.43404735e-03]]\n",
      "Cost after iteration 3800: [[2.34540846e-03 6.37251026e+00]\n",
      " [6.36504878e+00 2.33629074e-03]]\n",
      "Cost after iteration 3900: [[2.25412335e-03 6.41178911e+00]\n",
      " [6.40440934e+00 2.24548206e-03]]\n",
      "Cost after iteration 4000: [[2.16914243e-03 6.44983816e+00]\n",
      " [6.44253652e+00 2.16093892e-03]]\n",
      "Cost after iteration 4100: [[2.08983975e-03 6.48673759e+00]\n",
      " [6.47951079e+00 2.08203971e-03]]\n",
      "Cost after iteration 4200: [[2.01569719e-03 6.52254719e+00]\n",
      " [6.51539217e+00 2.00826979e-03]]\n",
      "Cost after iteration 4300: [[1.94623442e-03 6.55733170e+00]\n",
      " [6.55024553e+00 1.93915192e-03]]\n",
      "Cost after iteration 4400: [[1.88103451e-03 6.59114940e+00]\n",
      " [6.58412943e+00 1.87427205e-03]]\n",
      "Cost after iteration 4500: [[1.81974142e-03 6.62404420e+00]\n",
      " [6.61708792e+00 1.81327644e-03]]\n",
      "Cost after iteration 4600: [[1.76200966e-03 6.65607361e+00]\n",
      " [6.64917865e+00 1.75582178e-03]]\n",
      "Cost after iteration 4700: [[1.70756161e-03 6.68727257e+00]\n",
      " [6.68043673e+00 1.70163228e-03]]\n",
      "Cost after iteration 4800: [[1.65612049e-03 6.71769077e+00]\n",
      " [6.71091194e+00 1.65043285e-03]]\n",
      "Cost after iteration 4900: [[1.60746385e-03 6.74735754e+00]\n",
      " [6.74063379e+00 1.60200250e-03]]\n"
     ]
    }
   ],
   "source": [
    "parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 5000, print_cost = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "\n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "\n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "\n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "\n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "predictions_train = predict(train_x, train_y, parameters);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "predictions_test = predict(test_x, test_y, parameters);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [train_x.shape[0], 50, 25, 10, 2] #  5-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_layer_model\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "\n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "\n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "\n",
    "    # Parameters initialization.\n",
    "    ### START CODE HERE ###\n",
    "    parameters = initialize_parameters_deep(layers_dims);\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        AL, caches =L_model_forward(X, parameters);\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(AL, Y);\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        grads = L_model_backward(AL, Y, caches);\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate);\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "           # print (\"Cost after iteration %i: %f\" %(i, cost));\n",
    "            if print_cost and i % 100 == 0:\n",
    "                costs.append(cost);\n",
    "\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs));\n",
    "    plt.ylabel('cost');\n",
    "    plt.xlabel('iterations (per tens)');\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate));\n",
    "    plt.show();\n",
    "\n",
    "    return parameters;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: [[0.69314763 0.6931474 ]\n",
      " [0.69314673 0.69314696]]\n",
      "Cost after iteration 100: [[0.69222085 0.69424669]\n",
      " [0.69424683 0.69222098]]\n",
      "Cost after iteration 200: [[0.69158463 0.69528923]\n",
      " [0.69528939 0.69158476]]\n",
      "Cost after iteration 300: [[0.69114744 0.69624366]\n",
      " [0.69624383 0.69114758]]\n",
      "Cost after iteration 400: [[0.69084696 0.69709687]\n",
      " [0.69709706 0.6908471 ]]\n",
      "Cost after iteration 500: [[0.69064041 0.69784686]\n",
      " [0.69784706 0.69064054]]\n",
      "Cost after iteration 600: [[0.69049838 0.698498  ]\n",
      " [0.69849822 0.69049851]]\n",
      "Cost after iteration 700: [[0.69040071 0.69905809]\n",
      " [0.69905833 0.69040083]]\n",
      "Cost after iteration 800: [[0.69033353 0.69953642]\n",
      " [0.69953668 0.69033364]]\n",
      "Cost after iteration 900: [[0.69028731 0.69994268]\n",
      " [0.69994292 0.69028741]]\n",
      "Cost after iteration 1000: [[0.69025551 0.7002862 ]\n",
      " [0.70028643 0.6902556 ]]\n",
      "Cost after iteration 1100: [[0.69023363 0.70057566]\n",
      " [0.70057587 0.69023371]]\n",
      "Cost after iteration 1200: [[0.69021857 0.70081888]\n",
      " [0.70081907 0.69021865]]\n",
      "Cost after iteration 1300: [[0.6902082  0.70102278]\n",
      " [0.70102296 0.69020827]]\n",
      "Cost after iteration 1400: [[0.69020106 0.70119341]\n",
      " [0.70119358 0.69020113]]\n",
      "Cost after iteration 1500: [[0.69019614 0.70133599]\n",
      " [0.70133614 0.69019621]]\n",
      "Cost after iteration 1600: [[0.69019276 0.70145498]\n",
      " [0.70145511 0.69019282]]\n",
      "Cost after iteration 1700: [[0.69019042 0.70155418]\n",
      " [0.70155431 0.69019049]]\n",
      "Cost after iteration 1800: [[0.69018882 0.70163682]\n",
      " [0.70163693 0.69018888]]\n",
      "Cost after iteration 1900: [[0.69018771 0.70170561]\n",
      " [0.70170572 0.69018776]]\n",
      "Cost after iteration 2000: [[0.69018694 0.70176285]\n",
      " [0.70176295 0.69018699]]\n",
      "Cost after iteration 2100: [[0.69018641 0.70181046]\n",
      " [0.70181055 0.69018646]]\n",
      "Cost after iteration 2200: [[0.69018604 0.70185003]\n",
      " [0.70185011 0.69018609]]\n",
      "Cost after iteration 2300: [[0.69018579 0.70188292]\n",
      " [0.70188299 0.69018583]]\n",
      "Cost after iteration 2400: [[0.69018561 0.70191025]\n",
      " [0.70191032 0.69018565]]\n",
      "Cost after iteration 2500: [[0.69018548 0.70193295]\n",
      " [0.70193301 0.69018553]]\n",
      "Cost after iteration 2600: [[0.6901854  0.7019518 ]\n",
      " [0.70195186 0.69018543]]\n",
      "Cost after iteration 2700: [[0.69018533 0.70196746]\n",
      " [0.70196751 0.69018537]]\n",
      "Cost after iteration 2800: [[0.69018529 0.70198046]\n",
      " [0.70198051 0.69018532]]\n",
      "Cost after iteration 2900: [[0.69018525 0.70199126]\n",
      " [0.7019913  0.69018529]]\n",
      "Cost after iteration 3000: [[0.69018523 0.70200022]\n",
      " [0.70200026 0.69018526]]\n",
      "Cost after iteration 3100: [[0.69018521 0.70200766]\n",
      " [0.7020077  0.69018524]]\n",
      "Cost after iteration 3200: [[0.6901852  0.70201384]\n",
      " [0.70201387 0.69018522]]\n",
      "Cost after iteration 3300: [[0.69018518 0.70201897]\n",
      " [0.702019   0.69018521]]\n",
      "Cost after iteration 3400: [[0.69018517 0.70202323]\n",
      " [0.70202325 0.69018519]]\n",
      "Cost after iteration 3500: [[0.69018517 0.70202676]\n",
      " [0.70202678 0.69018518]]\n",
      "Cost after iteration 3600: [[0.69018516 0.7020297 ]\n",
      " [0.70202972 0.69018517]]\n",
      "Cost after iteration 3700: [[0.69018515 0.70203214]\n",
      " [0.70203215 0.69018516]]\n",
      "Cost after iteration 3800: [[0.69018514 0.70203416]\n",
      " [0.70203417 0.69018515]]\n",
      "Cost after iteration 3900: [[0.69018513 0.70203584]\n",
      " [0.70203585 0.69018514]]\n",
      "Cost after iteration 4000: [[0.69018513 0.70203724]\n",
      " [0.70203725 0.69018513]]\n",
      "Cost after iteration 4100: [[0.69018512 0.7020384 ]\n",
      " [0.70203841 0.69018512]]\n",
      "Cost after iteration 4200: [[0.69018511 0.70203937]\n",
      " [0.70203937 0.69018512]]\n",
      "Cost after iteration 4300: [[0.69018511 0.70204017]\n",
      " [0.70204017 0.69018511]]\n",
      "Cost after iteration 4400: [[0.6901851  0.70204083]\n",
      " [0.70204083 0.6901851 ]]\n",
      "Cost after iteration 4500: [[0.69018509 0.70204139]\n",
      " [0.70204139 0.69018509]]\n",
      "Cost after iteration 4600: [[0.69018509 0.70204185]\n",
      " [0.70204185 0.69018508]]\n",
      "Cost after iteration 4700: [[0.69018508 0.70204223]\n",
      " [0.70204223 0.69018508]]\n",
      "Cost after iteration 4800: [[0.69018507 0.70204255]\n",
      " [0.70204255 0.69018507]]\n",
      "Cost after iteration 4900: [[0.69018507 0.70204282]\n",
      " [0.70204281 0.69018506]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y can be no greater than 2-D, but have shapes (50,) and (50, 2, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-406-2d528e3e982c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL_layer_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-405-74dc20da19eb>\u001b[0m in \u001b[0;36mL_layer_model\u001b[1;34m(X, Y, layers_dims, learning_rate, num_iterations, print_cost)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# plot the cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcosts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cost'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'iterations (per tens)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2793\u001b[0m     return gca().plot(\n\u001b[0;32m   2794\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[1;32m-> 2795\u001b[1;33m         is not None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2796\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2797\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1664\u001b[0m         \"\"\"\n\u001b[0;32m   1665\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1666\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1667\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1668\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'plot'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n\u001b[1;32m--> 273\u001b[1;33m                              \"shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y can be no greater than 2-D, but have shapes (50,) and (50, 2, 2)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations =5000, print_cost = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(train_x, train_y, parameters);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "pred_test = predict(test_x, test_y, parameters);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
